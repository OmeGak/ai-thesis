@article{Deng2014,
abstract = {Deep learning, Machine learning, Artificial intelligence, Neural networks, Deep neural networks, Deep stacking networks, Autoencoders, Supervised learning, Unsupervised learning, Hybrid deep networks, Object recognition, Computer vision, Natural language processing, Language models, Multi-task learning, Multi-modal processing},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
eprint = {1309.1501},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/1601988141 [Deng {\&} Yu] Deep Learning - Methods and Applications.2014@CR.pdf:pdf},
isbn = {1601988141},
issn = {09598138},
number = {2013},
pages = {206},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
url = {http://books.google.com/books?id=46qNoAEACAAJ{\{}{\&}{\}}pgis=1},
volume = {7},
year = {2014}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3â€“7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/1508.06576v2.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {eural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph{\{}per-pixel{\}} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph{\{}perceptual{\}} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/1603.08155v1.pdf:pdf},
journal = {Arxiv},
keywords = {deep learning,style transfer,super-resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {http://arxiv.org/abs/1603.08155},
year = {2016}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/long{\_}shelhamer{\_}fcn.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Ruder2016,
abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
archivePrefix = {arXiv},
arxivId = {1604.08610},
author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1604.08610},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/1604.08610v1.pdf:pdf},
pages = {1--14},
title = {{Artistic style transfer for videos}},
url = {http://arxiv.org/abs/1604.08610},
year = {2016}
}
@article{Tang2010,
abstract = {A laser-triangulating range camera uses a laser plane to light an object. If the position of the laser relative to the camera as well as certrain properties of the camera is known, it is possible to calculate the coordinates for all points along the profile of the object. If either the object or the camera and laser has a known motion, it is possible to combine several measurements to get a three-dimensional view of the object. Camera calibration is the process of finding the properties of the camera and enough information about the setup so that the desired coordinates can be calcu- lated. Several methods for camera calibration exist, but this thesis proposes a new method that has the advantages that the objects needed are relatively inexpensive and that only objects in the laser plane need to be observed. Each part of the method is given a thorough description. Several mathematical derivations have also been added as appendices for completeness. The proposed method is tested using both synthetic and real data. The results show that the method is suitable even when high accuracy is needed. A few suggestions are also made about how the method can be improved further.},
author = {Tang, Zhiqiang},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/FULLTEXT01.pdf:pdf},
journal = {Electrical Engineering},
number = {1},
pages = {13--34},
title = {{Institutionen f{\"{o}}r systemteknik Department of Electrical Engineering}},
url = {http://www.vehicular.isy.liu.se/Publications/MSc/09{\_}EX{\_}4227{\_}JL.pdf},
volume = {54},
year = {2010}
}
@article{Upchurch2016,
abstract = {We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4{\%} lower dissimilarity to the ground truth than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1603.02003},
author = {Upchurch, Paul and Snavely, Noah and Bala, Kavita},
eprint = {1603.02003},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/1603.02003v1.pdf:pdf},
title = {{From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators}},
url = {http://arxiv.org/abs/1603.02003},
year = {2016}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/omegak/Dropbox/{\#}School/6.SoftComputing/PT/Papers/1506.06579v1.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
