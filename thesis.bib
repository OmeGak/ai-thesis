@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:Users/omegak/Documents/Papers/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
journal = {None},
mendeley-groups = {Thesis,Thesis/Used},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Bergstra2010,
abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy's syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy's syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
author = {Bergstra, James and Breuleux, Olivier and Bastien, Frederic Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
file = {:Users/omegak/Documents/Papers/Bergstra et al. - 2010 - Theano a CPU and GPU math compiler in Python.pdf:pdf},
journal = {Proceedings of the Python for Scientific Computing Conference (SciPy)},
mendeley-groups = {Thesis,Thesis/Used},
number = {Scipy},
pages = {1--7},
title = {{Theano: a CPU and GPU math compiler in Python}},
url = {http://www-etud.iro.umontreal.ca/$\sim$wardefar/publications/theano_scipy2010.pdf},
year = {2010}
}
@article{Collobert2002,
abstract = {Many scientific communities have expressed a growing interest in machine learning algorithms recently, mainly due to the generally good results they provide, compared to traditional statistical or AI approaches. However, these machine learning algorithms are often complex to implement and to use properly and efficiently. We thus present in this paper a new machine learning software library in which most state-of-the-art algorithms have already been implemented and are available in a unified framework, in order for scientists to be able to use them, compare them, and even extend them. More interestingly, this library is freely available under a BSD license and can be retrieved on the web by everyone.},
author = {Collobert, Ronan and Bengio, Samy and Mariethoz, Johnny},
file = {:Users/omegak/Documents/Papers/Collobert, Bengio, Mariethoz - 2002 - Torch A Modular Machine Learning Software Library.pdf:pdf},
isbn = {IDIAP-RR 02-46},
mendeley-groups = {Thesis,Thesis/Used},
pages = {7},
title = {{Torch: A Modular Machine Learning Software Library}},
year = {2002}
}
@article{Decaudin1996,
author = {Decaudin, Philippe},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/decaudin_1996.pdf:pdf},
journal = {Syntim Project Inria},
keywords = {cartoon,cel shading,computer graphics,non-photorealistic rendering},
mendeley-groups = {Thesis,Used in thesis},
number = {June},
pages = {1--11},
title = {{Cartoon-looking rendering of 3D-scenes}},
url = {ftp://meria.idc.ac.il/Faculty/arik/LODSeminar/07Shading/decaudin_1996.pdf},
year = {1996}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/imagenet_cvpr09.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Thesis,Used in thesis},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Deng2014,
abstract = {Deep learning, Machine learning, Artificial intelligence, Neural networks, Deep neural networks, Deep stacking networks, Autoencoders, Supervised learning, Unsupervised learning, Hybrid deep networks, Object recognition, Computer vision, Natural language processing, Language models, Multi-task learning, Multi-modal processing},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
eprint = {1309.1501},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1601988141 [Deng & Yu] Deep Learning - Methods and Applications.2014@CR.pdf:pdf},
isbn = {1601988141},
issn = {09598138},
number = {2013},
pages = {206},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
url = {http://books.google.com/books?id=46qNoAEACAAJ{&}pgis=1},
volume = {7},
year = {2014}
}
@misc{Esaak,
author = {Esaak, Shelley},
title = {{What is the Definition of Art?}},
url = {http://arthistory.about.com/cs/reference/f/what_is_art.htm},
urldate = {2016-07-14}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/1508.06576v2.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {neural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Hubel1968,
abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded. 2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent. 3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other. 4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven. 5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
author = {Hubel, D. H. and Wiesel, T N},
doi = {papers://47831562-1F78-4B52-B52E-78BF7F97A700/Paper/p352},
file = {:Users/omegak/Documents/Papers/Hubel, Wiesel - 1968 - Receptive Fields and Functional Architecture of monkey striate cortex.pdf:pdf},
isbn = {0022-3751 (Print) 0022-3751 (Linking)},
issn = {0022-3751},
journal = {Journal of Physiology},
keywords = {classics,macaque (rhesus) monkey,visual cortex,visual system},
mendeley-groups = {Thesis,Thesis/Used},
pages = {215--243},
pmid = {4966457},
title = {{Receptive Fields and Functional Architecture of monkey striate cortex}},
volume = {195},
year = {1968}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor and Eecs, U C Berkeley},
eprint = {arXiv:1408.5093v1},
file = {:Users/omegak/Documents/Papers/Jia et al. - 2014 - Convolutional_Architecture_Feature_Embedding.pdf:pdf},
isbn = {9781450330633},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
mendeley-groups = {Thesis,Thesis/Used},
title = {{Convolutional Architecture Feature Embedding}},
year = {2014}
}
@misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
}
@article{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1603.08155v1.pdf:pdf},
journal = {Arxiv},
keywords = {deep learning,style transfer,super-resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {http://arxiv.org/abs/1603.08155},
year = {2016}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Kyprianidis2013,
abstract = {This paper surveys the field of non-photorealistic rendering (NPR), focusing on techniques for transforming 2D input (images and video) into artistically stylized renderings. We first present a taxonomy of the 2D NPR algorithms developed over the past two decades, structured according to the design characteristics and behavior of each technique. We then describe a chronology of development from the semi-automatic paint systems of the early nineties, through to the automated painterly rendering systems of the late nineties driven by image gradient analysis. Two complementary trends in the NPR literature are then addressed, with reference to our taxonomy. First, the fusion of higher level computer vision and NPR, illustrating the trends toward scene analysis to drive artistic abstraction and diversity of style. Second, the evolution of local processing approaches toward edge-aware filtering for real-time stylization of images and video. The survey then concludes with a discussion of open challenges for 2D NPR identified in recent NPR symposia, including topics such as user and aesthetic evaluation.},
author = {Kyprianidis, Jan Eric and Collomosse, John and Wang, Tinghuai and Isenberg, Tobias},
doi = {10.1109/TVCG.2012.160},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/kyprianidis2013.pdf:pdf},
isbn = {10.1109/TVCG.2012.160},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Image and video stylization,artistic rendering,nonphotorealistic rendering (NPR)},
mendeley-groups = {Thesis,Used in thesis},
number = {5},
pages = {866--885},
pmid = {22802120},
title = {{State of the 'Art: A taxonomy of artistic stylization techniques for images and video}},
volume = {19},
year = {2013}
}
@article{Lawrence1997,
abstract = {We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.},
author = {Lawrence, S and Giles, C.L. and {Ah Chung Tsoi} and Back, A.D.},
doi = {10.1109/72.554195},
file = {:Users/omegak/Documents/Papers/Lawrence et al. - 1997 - Face recognition a convolutional neural-network approach.pdf:pdf},
isbn = {1045-9227 VO - 8},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
mendeley-groups = {Thesis,Thesis/Used},
number = {1},
pages = {98--113},
pmid = {18255614},
title = {{Face recognition: a convolutional neural-network approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=554195},
volume = {8},
year = {1997}
}
@article{LeCun1995,
abstract = {The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classifier then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classifiers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with "raw" inputs (e.g. normalized images), and to rely on backpropagation to turn the first few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {LeCun, Y and Bengio, Y},
doi = {10.1109/IJCNN.2004.1381049},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/LeCun, Bengio - 1995 - Convolutional networks for images, speech, and time series.pdf:pdf},
isbn = {0262511029},
issn = {1098-7576},
journal = {The handbook of brain theory and neural networks},
mendeley-groups = {Thesis,Thesis/Used},
number = {April 2016},
pages = {255--258},
pmid = {17001990},
title = {{Convolutional networks for images, speech, and time series}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9297&rep=rep1&type=pdf},
volume = {3361},
year = {1995}
}
@article{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/omegak/Documents/Papers/LeCun et al. - 1998 - Gradient Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {convo-,document recogni-,fi-,gradient-based learning,graph transformer networks,lutional neural networks,machine learning,neural networks,nite state transducers,ocr,tion},
mendeley-groups = {Thesis,Thesis/Used},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/lin2011.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
mendeley-groups = {Thesis,Used in thesis},
number = {4},
pages = {297--312},
publisher = {Elsevier Inc.},
title = {{Perceptual visual quality metrics: A survey}},
url = {http://dx.doi.org/10.1016/j.jvcir.2011.01.005},
volume = {22},
year = {2011}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/long_shelhamer_fcn.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Ruder2016,
abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
archivePrefix = {arXiv},
arxivId = {1604.08610},
author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1604.08610},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1604.08610v1.pdf:pdf},
pages = {1--14},
title = {{Artistic style transfer for videos}},
url = {http://arxiv.org/abs/1604.08610},
year = {2016}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.0575v3.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Tang2010,
abstract = {A laser-triangulating range camera uses a laser plane to light an object. If the position of the laser relative to the camera as well as certrain properties of the camera is known, it is possible to calculate the coordinates for all points along the profile of the object. If either the object or the camera and laser has a known motion, it is possible to combine several measurements to get a three-dimensional view of the object. Camera calibration is the process of finding the properties of the camera and enough information about the setup so that the desired coordinates can be calcu- lated. Several methods for camera calibration exist, but this thesis proposes a new method that has the advantages that the objects needed are relatively inexpensive and that only objects in the laser plane need to be observed. Each part of the method is given a thorough description. Several mathematical derivations have also been added as appendices for completeness. The proposed method is tested using both synthetic and real data. The results show that the method is suitable even when high accuracy is needed. A few suggestions are also made about how the method can be improved further.},
author = {Tang, Zhiqiang},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/FULLTEXT01.pdf:pdf},
journal = {Electrical Engineering},
number = {1},
pages = {13--34},
title = {{Institutionen f{\"{o}}r systemteknik Department of Electrical Engineering}},
url = {http://www.vehicular.isy.liu.se/Publications/MSc/09_EX_4227_JL.pdf},
volume = {54},
year = {2010}
}
@article{Tenenbaum1997,
abstract = {We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor struc- ture. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMsj, which generalizes earlier work on factorial mixture models [7, 3]. Significant per- formance improvement on a benchmark speech dataset shows the benefits of our approach.},
author = {Tenenbaum, J B and Freeman, W T},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1290-separating-style-and-content.pdf:pdf},
journal = {Advances in neural information processing},
title = {{Separating style and content with bilinear models.}},
volume = {9},
year = {1997}
}
@article{Upchurch2016,
abstract = {We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4% lower dissimilarity to the ground truth than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1603.02003},
author = {Upchurch, Paul and Snavely, Noah and Bala, Kavita},
eprint = {1603.02003},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1603.02003v1.pdf:pdf},
title = {{From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators}},
url = {http://arxiv.org/abs/1603.02003},
year = {2016}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1506.06579v1.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.1556.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {ImageNet Challenge},
mendeley-groups = {Thesis,Used in thesis},
pages = {1--10},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Tenenbaum2000,
abstract = {Perceptual systems routinely separate "content" from "style," classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
author = {Tenenbaum, J B and Freeman, W T},
doi = {10.1162/089976600300015349},
file = {:Users/omegak/Desktop/10.1162@089976600300015349.pdf:pdf},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1247--1283},
pmid = {10935711},
title = {{Separating style and content with bilinear models.}},
volume = {12},
year = {2000}
}
@article{Wilkniss1998,
author = {Wilkniss, Sandra and Davis, Kristin},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/p292-lin.pdf:pdf},
isbn = {026206197X},
issn = {1095-158X},
journal = {Psychiatric rehabilitation journal},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {244--8},
pmid = {20061264},
title = {{Book reviews.WordNet: An Electronic Lexical Database}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20061264},
volume = {33},
year = {1998}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
mendeley-groups = {Thesis},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{ZHANG1999,
abstract = {In this paper, we report our experiments on feature-based facial expression recognition within an architecture based on a two-layer perceptron. We investigate the use of two types of features extracted from face images:  the geometric positions of a set of fiducial points on a face, and a set of multi-scale and multi-orientation Gabor wavelet coefficients at these points.  They can be used either independently or jointly. The recognition performance with different types of features has been compared, which shows that Gabor wavelet coefficients are much more powerful than geometric positions.  Furthermore, since the first layer of the perceptron actually performs a nonlinear reduction of the dimensionality of the feature space, we have also studied the desired number of hidden units, i.e., the appropriate dimension to represent a facial expression in order to achieve a good recognition rate.   It turns out that five to seven hidden units are probably enough to represent the space of feature expressions. Then, we have investigated the importance of each individual fiducial point to facial expression recognition.  Sensitivity analysis reveals that points on cheeks and on forehead carry little useful information.  After discarding them, not only the computational efficiency increases, but also the generalization performance slightly improves. Finally, we have studied the significance of image scales. Experiments show that facial expression recognition is mainly a low frequency process, and a spatial resolution of 64 pixels  64 pixels is probably enough},
author = {ZHANG, ZHENGYOU},
doi = {10.1142/S0218001499000495},
file = {:Users/omegak/Documents/Papers/ZHANG - 1999 - Feature-Based Facial Expression Recognition Sensitivity Analysis and Experiments With a Multilayer Perceptron.pdf:pdf},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {facial expression recognition,gabor wavelets,image scale,learning,multilayer perceptron,sensitivity analysis,},
mendeley-groups = {Thesis,Thesis/Used},
number = {06},
pages = {893--911},
title = {{Feature-Based Facial Expression Recognition: Sensitivity Analysis and Experiments With a Multilayer Perceptron}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001499000495},
volume = {13},
year = {1999}
}


% -- Images --------------------------------------------------------------------

@image{Aphex342015,
abstract = {Input volume connected to a convolutional layer},
author = {Aphex34},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/Aphex34 - 2015 - Convolutional Layer.html:html},
mendeley-groups = {Thesis/Images},
title = {{Convolutional Layer}},
url = {https://en.wikipedia.org/wiki/File:Conv_layer.png},
urldate = {2016-08-04},
year = {2015}
}
@misc{Guerzhoy2016,
author = {Guerzhoy, Michael},
booktitle = {University of Toronto},
file = {:Users/omegak/Documents/Papers/Guerzhoy - 2016 - Modern ConvNet Architectures.pdf:pdf},
mendeley-groups = {Thesis/Images},
title = {{Modern ConvNet Architectures}},
url = {http://www.cs.toronto.edu/~guerzhoy/321/lec/W06/convnets.pdf},
urldate = {2016-08-06},
year = {2016}
}
