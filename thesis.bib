@article{Decaudin1996,
author = {Decaudin, Philippe},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/decaudin_1996.pdf:pdf},
journal = {Syntim Project Inria},
keywords = {cartoon,cel shading,computer graphics,non-photorealistic rendering},
mendeley-groups = {Thesis,Used in thesis},
number = {June},
pages = {1--11},
title = {{Cartoon-looking rendering of 3D-scenes}},
url = {ftp://meria.idc.ac.il/Faculty/arik/LODSeminar/07Shading/decaudin_1996.pdf},
year = {1996}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/imagenet_cvpr09.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Thesis,Used in thesis},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Deng2014,
abstract = {Deep learning, Machine learning, Artificial intelligence, Neural networks, Deep neural networks, Deep stacking networks, Autoencoders, Supervised learning, Unsupervised learning, Hybrid deep networks, Object recognition, Computer vision, Natural language processing, Language models, Multi-task learning, Multi-modal processing},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
eprint = {1309.1501},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1601988141 [Deng & Yu] Deep Learning - Methods and Applications.2014@CR.pdf:pdf},
isbn = {1601988141},
issn = {09598138},
number = {2013},
pages = {206},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
url = {http://books.google.com/books?id=46qNoAEACAAJ{&}pgis=1},
volume = {7},
year = {2014}
}
@misc{Esaak,
author = {Esaak, Shelley},
title = {{What is the Definition of Art?}},
url = {http://arthistory.about.com/cs/reference/f/what_is_art.htm},
urldate = {2016-07-14}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3â€“7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/1508.06576v2.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {neural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor and Eecs, U C Berkeley},
eprint = {arXiv:1408.5093v1},
file = {:Users/omegak/Documents/Papers/Jia et al. - 2014 - Convolutional_Architecture_Feature_Embedding.pdf:pdf},
isbn = {9781450330633},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
mendeley-groups = {Thesis,Thesis/Used},
title = {{Convolutional Architecture Feature Embedding}},
year = {2014}
}
@misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
}
@article{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1603.08155v1.pdf:pdf},
journal = {Arxiv},
keywords = {deep learning,style transfer,super-resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {http://arxiv.org/abs/1603.08155},
year = {2016}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Kyprianidis2013,
abstract = {This paper surveys the field of non-photorealistic rendering (NPR), focusing on techniques for transforming 2D input (images and video) into artistically stylized renderings. We first present a taxonomy of the 2D NPR algorithms developed over the past two decades, structured according to the design characteristics and behavior of each technique. We then describe a chronology of development from the semi-automatic paint systems of the early nineties, through to the automated painterly rendering systems of the late nineties driven by image gradient analysis. Two complementary trends in the NPR literature are then addressed, with reference to our taxonomy. First, the fusion of higher level computer vision and NPR, illustrating the trends toward scene analysis to drive artistic abstraction and diversity of style. Second, the evolution of local processing approaches toward edge-aware filtering for real-time stylization of images and video. The survey then concludes with a discussion of open challenges for 2D NPR identified in recent NPR symposia, including topics such as user and aesthetic evaluation.},
author = {Kyprianidis, Jan Eric and Collomosse, John and Wang, Tinghuai and Isenberg, Tobias},
doi = {10.1109/TVCG.2012.160},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/kyprianidis2013.pdf:pdf},
isbn = {10.1109/TVCG.2012.160},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Image and video stylization,artistic rendering,nonphotorealistic rendering (NPR)},
mendeley-groups = {Thesis,Used in thesis},
number = {5},
pages = {866--885},
pmid = {22802120},
title = {{State of the 'Art: A taxonomy of artistic stylization techniques for images and video}},
volume = {19},
year = {2013}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/lin2011.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
mendeley-groups = {Thesis,Used in thesis},
number = {4},
pages = {297--312},
publisher = {Elsevier Inc.},
title = {{Perceptual visual quality metrics: A survey}},
url = {http://dx.doi.org/10.1016/j.jvcir.2011.01.005},
volume = {22},
year = {2011}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/long_shelhamer_fcn.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Ruder2016,
abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
archivePrefix = {arXiv},
arxivId = {1604.08610},
author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1604.08610},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1604.08610v1.pdf:pdf},
pages = {1--14},
title = {{Artistic style transfer for videos}},
url = {http://arxiv.org/abs/1604.08610},
year = {2016}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.0575v3.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Tang2010,
abstract = {A laser-triangulating range camera uses a laser plane to light an object. If the position of the laser relative to the camera as well as certrain properties of the camera is known, it is possible to calculate the coordinates for all points along the profile of the object. If either the object or the camera and laser has a known motion, it is possible to combine several measurements to get a three-dimensional view of the object. Camera calibration is the process of finding the properties of the camera and enough information about the setup so that the desired coordinates can be calcu- lated. Several methods for camera calibration exist, but this thesis proposes a new method that has the advantages that the objects needed are relatively inexpensive and that only objects in the laser plane need to be observed. Each part of the method is given a thorough description. Several mathematical derivations have also been added as appendices for completeness. The proposed method is tested using both synthetic and real data. The results show that the method is suitable even when high accuracy is needed. A few suggestions are also made about how the method can be improved further.},
author = {Tang, Zhiqiang},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/FULLTEXT01.pdf:pdf},
journal = {Electrical Engineering},
number = {1},
pages = {13--34},
title = {{Institutionen f{\"{o}}r systemteknik Department of Electrical Engineering}},
url = {http://www.vehicular.isy.liu.se/Publications/MSc/09_EX_4227_JL.pdf},
volume = {54},
year = {2010}
}
@article{Tenenbaum1997,
abstract = {We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor struc- ture. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMsj, which generalizes earlier work on factorial mixture models [7, 3]. Significant per- formance improvement on a benchmark speech dataset shows the benefits of our approach.},
author = {Tenenbaum, J B and Freeman, W T},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1290-separating-style-and-content.pdf:pdf},
journal = {Advances in neural information processing},
title = {{Separating style and content with bilinear models.}},
volume = {9},
year = {1997}
}
@article{Upchurch2016,
abstract = {We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4% lower dissimilarity to the ground truth than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1603.02003},
author = {Upchurch, Paul and Snavely, Noah and Bala, Kavita},
eprint = {1603.02003},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1603.02003v1.pdf:pdf},
title = {{From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators}},
url = {http://arxiv.org/abs/1603.02003},
year = {2016}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1506.06579v1.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.1556.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {ImageNet Challenge},
mendeley-groups = {Thesis,Used in thesis},
pages = {1--10},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Tenenbaum2000,
abstract = {Perceptual systems routinely separate "content" from "style," classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
author = {Tenenbaum, J B and Freeman, W T},
doi = {10.1162/089976600300015349},
file = {:Users/omegak/Desktop/10.1162@089976600300015349.pdf:pdf},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1247--1283},
pmid = {10935711},
title = {{Separating style and content with bilinear models.}},
volume = {12},
year = {2000}
}
@article{Wilkniss1998,
author = {Wilkniss, Sandra and Davis, Kristin},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/p292-lin.pdf:pdf},
isbn = {026206197X},
issn = {1095-158X},
journal = {Psychiatric rehabilitation journal},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {244--8},
pmid = {20061264},
title = {{Book reviews.WordNet: An Electronic Lexical Database}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20061264},
volume = {33},
year = {1998}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
mendeley-groups = {Thesis},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
