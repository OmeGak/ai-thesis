@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:Users/omegak/Documents/Papers/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
journal = {None},
mendeley-groups = {Thesis,Thesis/Used},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Alonso2008,
author = {Alonso, Jose-Manuel and Chen, Yao},
doi = {doi:10.4249/scholarpedia.5393},
file = {:Users/omegak/Documents/Papers/Alonso, Chen - 2008 - Receptive Field.html:html},
journal = {Shcolarpedia},
mendeley-groups = {Thesis/Used},
number = {1},
pages = {5393},
title = {{Receptive Field}},
url = {http://www.scholarpedia.org/article/Receptive_field},
volume = {4},
year = {2008}
}
@article{Baker2009,
abstract = {This article is the second part of an updated version of the "MINDS 2006-2007 Report of the Speech Understanding Working Group," one of five reports emanating from two workshops entitled "Meeting of the MINDS: Future Directions for Human Language Technology," sponsored by the U.S. Disruptive Technology Office (DTO). (MINDS is an acronym for "machine translation, information retrieval, natural-language processing, data resources, and speech understanding").},
author = {Baker, Janet M. and Deng, Li and Khudanpur, Sanjeev and Lee, Chin Hui and Glass, James R. and Morgan, Nelson and O'Shaughnessy, Douglas},
doi = {10.1109/MSP.2009.932707},
file = {:Users/omegak/Documents/Papers/Baker et al. - 2009 - Updated MINDS report on speech recognition and understanding.pdf:pdf},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
mendeley-groups = {Thesis/Articles},
number = {4},
pages = {78--85},
pmid = {19055118},
title = {{Updated MINDS report on speech recognition and understanding}},
volume = {26},
year = {2009}
}
@article{Bergstra2010,
abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy's syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy's syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
author = {Bergstra, James and Breuleux, Olivier and Bastien, Frederic Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
file = {:Users/omegak/Documents/Papers/Bergstra et al. - 2010 - Theano a CPU and GPU math compiler in Python.pdf:pdf},
journal = {Proceedings of the Python for Scientific Computing Conference (SciPy)},
mendeley-groups = {Thesis,Thesis/Used},
number = {Scipy},
pages = {1--7},
title = {{Theano: a CPU and GPU math compiler in Python}},
url = {http://www-etud.iro.umontreal.ca/~wardefar/publications/theano_scipy2010.pdf},
year = {2010}
}
@misc{Bernacki2005,
author = {Bernacki, Mariusz and W{\l}odarczyk, Przemys{\l}aw},
booktitle = {Katedra Elektroniki AGH},
file = {:Users/omegak/Documents/Papers/Bernacki, W{\l}odarczyk - 2005 - Backpropagation.html:html},
mendeley-groups = {Thesis/Articles},
title = {{Backpropagation}},
url = {http://home.agh.edu.pl/{~}vlsi/AI/backp{\_}t{\_}en/backprop.html},
urldate = {2016-08-21},
year = {2005}
}
@article{Collobert2002,
abstract = {Many scientific communities have expressed a growing interest in machine learning algorithms recently, mainly due to the generally good results they provide, compared to traditional statistical or AI approaches. However, these machine learning algorithms are often complex to implement and to use properly and efficiently. We thus present in this paper a new machine learning software library in which most state-of-the-art algorithms have already been implemented and are available in a unified framework, in order for scientists to be able to use them, compare them, and even extend them. More interestingly, this library is freely available under a BSD license and can be retrieved on the web by everyone.},
author = {Collobert, Ronan and Bengio, Samy and Mariethoz, Johnny},
file = {:Users/omegak/Documents/Papers/Collobert, Bengio, Mariethoz - 2002 - Torch A Modular Machine Learning Software Library.pdf:pdf},
isbn = {IDIAP-RR 02-46},
mendeley-groups = {Thesis,Thesis/Used},
pages = {7},
title = {{Torch: A Modular Machine Learning Software Library}},
year = {2002}
}
@article{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm},
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
file = {:Users/omegak/Documents/Papers/Dean et al. - Unknown - Large Scale Distributed Deep Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {NIPS 2012: Neural Information Processing Systems},
mendeley-groups = {Thesis/Articles},
pages = {1--11},
pmid = {43479959},
title = {{Large Scale Distributed Deep Networks}},
year = {2012}
}
@article{Chen2012,
abstract = {The Context-Dependent Deep-Neural-Network HMM, or CD- DNN-HMM, is a recently proposed acoustic-modeling tech- nique for HMM-based speech recognition that can greatly out- perform conventional Gaussian-mixture based HMMs. For ex- ample, a CD-DNN-HMM trained on the 2000h Fisher corpus achieves 14.4{\%} word error rate on the Hub5'00-FSH speaker- independent phone-call transcription task, compared to 19.6{\%} obtained by a state-of-the-art, conventional discriminatively trained GMM-based HMM. That CD-DNN-HMM, however, took 59 days to train on a modern GPGPU—the immense computational cost of the mini- batch based back-propagation (BP) training is a major road- block. Unlike the familiar Baum-Welch training for conven- tional HMMs, BP cannot be efficiently parallelized across data. In this paper we show that the pipelined approximation to BP, which parallelizes computation with respect to layers, is an efficient way of utilizing multiple GPGPU cards in a single server. Using 2 and 4 GPGPUs, we achieve a 1.9 and 3.3 times end-to-end speed-up, at parallelization efficiency of 0.95 and 0.82, respectively, at no loss of recognition accuracy.},
author = {Chen, Xie and Eversole, Adam and Li, Gang and Yu, Dong and Seide, Frank},
file = {:Users/omegak/Documents/Papers/Chen et al. - Unknown - Pipelined Back-Propagation for Context-Dependent Deep Neural Networks.pdf:pdf},
isbn = {9781622767595},
journal = {Proc. Interspeech},
mendeley-groups = {Thesis/Articles},
pages = {2--5},
title = {{Pipelined back-propagation for context-dependent deep neural networks}},
url = {http://research.microsoft.com/pubs/173312/DNN-Pipeline-Interspeech2012.pdf},
year = {2012}
}
@article{Decaudin1996,
author = {Decaudin, Philippe},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/decaudin_1996.pdf:pdf},
journal = {Syntim Project Inria},
keywords = {cartoon,cel shading,computer graphics,non-photorealistic rendering},
mendeley-groups = {Thesis,Used in thesis},
number = {June},
pages = {1--11},
title = {{Cartoon-looking rendering of 3D-scenes}},
url = {ftp://meria.idc.ac.il/Faculty/arik/LODSeminar/07Shading/decaudin_1996.pdf},
year = {1996}
}
@article{Deng1999,
abstract = {Major speech production models from speech science literature and a number of popular statistical " generative " models of speech used in speech technology are surveyed. Strengths and weaknesses of these two styles of speech models are analyzed, pointing to the need to integrate the respective strengths while eliminating the respective weaknesses. As an example, a statistical task-dynamic model of speech production is described, motivated by the original deterministic version of the model and targeted for integrated-multilingual speech recognition applications. Methods for model parameter learning (training) and for likelihood computation (recognition) are described based on statistical optimization princi-ples integrated in neural network and dynamic system theories.},
author = {Deng, Li},
file = {:Users/omegak/Documents/Papers/Deng - 1999 - Computational models for speech production.pdf:pdf},
journal = {Computational models of speech pattern processing},
mendeley-groups = {Thesis/Articles},
pages = {199--213},
title = {{Computational models for speech production}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-60087-6{\_}20},
year = {1999}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/imagenet_cvpr09.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Thesis,Used in thesis},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Deng2014,
abstract = {Deep learning, Machine learning, Artificial intelligence, Neural networks, Deep neural networks, Deep stacking networks, Autoencoders, Supervised learning, Unsupervised learning, Hybrid deep networks, Object recognition, Computer vision, Natural language processing, Language models, Multi-task learning, Multi-modal processing},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
eprint = {1309.1501},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1601988141 [Deng & Yu] Deep Learning - Methods and Applications.2014@CR.pdf:pdf},
isbn = {1601988141},
issn = {09598138},
number = {2013},
pages = {206},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
url = {http://books.google.com/books?id=46qNoAEACAAJ&pgis=1},
volume = {7},
year = {2014}
}
@misc{Esaak,
author = {Esaak, Shelley},
title = {{What is the Definition of Art?}},
url = {http://arthistory.about.com/cs/reference/f/what_is_art.htm},
urldate = {2016-07-14}
}
@article{Farley1954,
author = {Farley, BWAC and Clark, W},
journal = {Transactions of the IRE Professional Group on Information Theory},
mendeley-groups = {Thesis/Articles},
number = {4},
pages = {76--84},
title = {{Simulation of self-organizing systems by digital computer}},
volume = {4},
year = {1954}
}
@article{Freund1999,
abstract = {We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithmwith Helmbold andWarmuth's leave-one-outmethod. LikeVapnik's maximal margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximalmargin classifiers on the same problem, while saving significantly on computation time and programming effort},
author = {Freund, Yoav and Schapire, Robert E.},
doi = {10.1023/A:1007662407062},
file = {:Users/omegak/Documents/Papers/Freund, Schapire - 1999 - Large Margin Classification Using the Perceptron Algorithm.pdf:pdf},
isbn = {1581130570},
issn = {08856125},
journal = {Machine Learning},
mendeley-groups = {Thesis/Articles},
number = {3},
pages = {277--296},
title = {{Large margin classification using the perceptron algorithm}},
url = {http://cseweb.ucsd.edu/{~}yfreund/papers/LargeMarginsUsingPerceptron.pdf},
volume = {37},
year = {1999}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:Users/omegak/Documents/Papers/Fukushima - 1980 - Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in pos.pdf:pdf},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
mendeley-groups = {Thesis/Used},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/1508.06576v2.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {neural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Hinton1990,
author = {Hinton, Geoffrey I},
file = {:Users/omegak/Documents/Papers/Hinton - Unknown - Connectionist Learning Procedures.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Machine Learning -- an Artificial Intelligence Approach},
mendeley-groups = {Thesis/Articles},
number = {1989},
pages = {555--610},
title = {{Connectionist Learning Procedures}},
volume = {III},
year = {1990}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:Users/omegak/Documents/Papers/Hinton - 2006 - Communicated by Yann Le Cun A Fast Learning Algorithm for Deep Belief Nets 500 units 500 units.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
mendeley-groups = {Thesis/Articles},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
mendeley-groups = {Thesis/Used},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Hubel1968,
abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded. 2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent. 3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other. 4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven. 5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
author = {Hubel, D. H. and Wiesel, T N},
doi = {papers://47831562-1F78-4B52-B52E-78BF7F97A700/Paper/p352},
file = {:Users/omegak/Documents/Papers/Hubel, Wiesel - 1968 - Receptive Fields and Functional Architecture of monkey striate cortex.pdf:pdf},
isbn = {0022-3751 (Print) 0022-3751 (Linking)},
issn = {0022-3751},
journal = {Journal of Physiology},
keywords = {classics,macaque (rhesus) monkey,visual cortex,visual system},
mendeley-groups = {Thesis,Thesis/Used},
pages = {215--243},
pmid = {4966457},
title = {{Receptive Fields and Functional Architecture of monkey striate cortex}},
volume = {195},
year = {1968}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor and Eecs, U C Berkeley},
eprint = {arXiv:1408.5093v1},
file = {:Users/omegak/Documents/Papers/Jia et al. - 2014 - Convolutional_Architecture_Feature_Embedding.pdf:pdf},
isbn = {9781450330633},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
mendeley-groups = {Thesis,Thesis/Used},
title = {{Convolutional Architecture Feature Embedding}},
year = {2014}
}
@misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
}
@article{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1603.08155v1.pdf:pdf},
journal = {Arxiv},
keywords = {deep learning,style transfer,super-resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {http://arxiv.org/abs/1603.08155},
year = {2016}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Kyprianidis2013,
abstract = {This paper surveys the field of non-photorealistic rendering (NPR), focusing on techniques for transforming 2D input (images and video) into artistically stylized renderings. We first present a taxonomy of the 2D NPR algorithms developed over the past two decades, structured according to the design characteristics and behavior of each technique. We then describe a chronology of development from the semi-automatic paint systems of the early nineties, through to the automated painterly rendering systems of the late nineties driven by image gradient analysis. Two complementary trends in the NPR literature are then addressed, with reference to our taxonomy. First, the fusion of higher level computer vision and NPR, illustrating the trends toward scene analysis to drive artistic abstraction and diversity of style. Second, the evolution of local processing approaches toward edge-aware filtering for real-time stylization of images and video. The survey then concludes with a discussion of open challenges for 2D NPR identified in recent NPR symposia, including topics such as user and aesthetic evaluation.},
author = {Kyprianidis, Jan Eric and Collomosse, John and Wang, Tinghuai and Isenberg, Tobias},
doi = {10.1109/TVCG.2012.160},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/kyprianidis2013.pdf:pdf},
isbn = {10.1109/TVCG.2012.160},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Image and video stylization,artistic rendering,nonphotorealistic rendering (NPR)},
mendeley-groups = {Thesis,Used in thesis},
number = {5},
pages = {866--885},
pmid = {22802120},
title = {{State of the 'Art: A taxonomy of artistic stylization techniques for images and video}},
volume = {19},
year = {2013}
}
@misc{LaboratoiredInformatiquedesSystemesAdaptatifs2010,
author = {{Laboratoire d'Informatique des Syst{\`{e}}mes Adaptatifs}},
booktitle = {deeplearning.net},
file = {:Users/omegak/Documents/Papers/Laboratoire d'Informatique des Syst{\`{e}}mes Adaptatifs - 2010 - LeNet Architecture.png:png},
mendeley-groups = {Thesis/Images},
title = {{LeNet Architecture}},
url = {http://deeplearning.net/tutorial/lenet.html},
urldate = {2016-08-08},
year = {2010}
}
@article{Lawrence1997,
abstract = {We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.},
author = {Lawrence, S and Giles, C.L. and {Ah Chung Tsoi} and Back, A.D.},
doi = {10.1109/72.554195},
file = {:Users/omegak/Documents/Papers/Lawrence et al. - 1997 - Face recognition a convolutional neural-network approach.pdf:pdf},
isbn = {1045-9227 VO - 8},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
mendeley-groups = {Thesis,Thesis/Used},
number = {1},
pages = {98--113},
pmid = {18255614},
title = {{Face recognition: a convolutional neural-network approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=554195},
volume = {8},
year = {1997}
}
@article{LeCun1995,
abstract = {The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classifier then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classifiers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with "raw" inputs (e.g. normalized images), and to rely on backpropagation to turn the first few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {LeCun, Y and Bengio, Y},
doi = {10.1109/IJCNN.2004.1381049},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/LeCun, Bengio - 1995 - Convolutional networks for images, speech, and time series.pdf:pdf},
isbn = {0262511029},
issn = {1098-7576},
journal = {The handbook of brain theory and neural networks},
mendeley-groups = {Thesis,Thesis/Used},
number = {April 2016},
pages = {255--258},
pmid = {17001990},
title = {{Convolutional networks for images, speech, and time series}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9297&rep=rep1&type=pdf},
volume = {3361},
year = {1995}
}
@article{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/omegak/Documents/Papers/LeCun et al. - 1998 - Gradient Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {convo-,document recogni-,fi-,gradient-based learning,graph transformer networks,lutional neural networks,machine learning,neural networks,nite state transducers,ocr,tion},
mendeley-groups = {Thesis,Thesis/Used},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@article{LeCun2004,
author = {{Le Cun}, Leon Bottou Yann and Bottou, Leon},
journal = {Advances in neural information processing systems},
mendeley-groups = {Thesis/Articles},
pages = {217},
publisher = {The MIT Press},
title = {{Large scale online learning}},
volume = {16},
year = {2004}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/lin2011.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
mendeley-groups = {Thesis,Used in thesis},
number = {4},
pages = {297--312},
publisher = {Elsevier Inc.},
title = {{Perceptual visual quality metrics: A survey}},
url = {http://dx.doi.org/10.1016/j.jvcir.2011.01.005},
volume = {22},
year = {2011}
}
@article{Linnainmaa1976,
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
file = {:Users/omegak/Documents/Papers/Linnainmaa - 1976 - TAYLOR EXPANSION OF THE ACCUMULATED.pdf:pdf},
issn = {00063835},
journal = {Bit},
mendeley-groups = {Thesis/Articles},
number = {2},
pages = {146--160},
title = {{Taylor expansion of the accumulated rounding error}},
volume = {16},
year = {1976}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/long_shelhamer_fcn.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/McCulloch, Pitts - 1943 - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
mendeley-groups = {Thesis/Articles},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Minsky1969,
author = {Minsky, Marvin and Papert, Seymour},
mendeley-groups = {Thesis/Used},
publisher = {MIT press},
title = {{Perceptrons}},
year = {1969}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:Users/omegak/Documents/Papers/Hinton - Unknown - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
mendeley-groups = {Thesis/Used},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Parker1985,
author = {Parker, David B},
mendeley-groups = {Thesis/Used},
title = {{Learning logic}},
year = {1985}
}
@article{Riesenhuber1999,
abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
author = {Riesenhuber, M. and Poggio, T.},
doi = {10.1038/14819},
file = {:Users/omegak/Documents/Papers/Riesenhuber, Poggio - 1999 - Hierarchical models of object recognition in cortex.pdf:pdf},
isbn = {1097-6256},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Animals,Computer Simulation,Form Perception,Form Perception: physiology,Macaca,Mental Recall,Mental Recall: physiology,Models,Neurological,Neurons,Neurons: physiology,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Fields,Visual Fields: physiology},
mendeley-groups = {Thesis/Used},
number = {11},
pages = {1019--25},
pmid = {10526343},
title = {{Hierarchical models of object recognition in cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10526343},
volume = {2},
year = {1999}
}
@book{Rosenblatt1957,
author = {Rosenblatt, Frank},
mendeley-groups = {Thesis/Articles},
publisher = {Cornell Aeronautical Laboratory},
title = {{The perceptron, a perceiving and recognizing automaton Project Para}},
year = {1957}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {:Users/omegak/Documents/Papers/Rosenblatt - 1958 - The perceptron A probabilistic model for information storage and organization in {\ldots}.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
mendeley-groups = {Thesis/Articles},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf$\backslash$npapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@article{Ruck1990,
author = {Ruck, Dw and Rogers, Sk and Kabrisky, M},
file = {:Users/omegak/Documents/Papers/Ruck, Rogers, Kabrisky - 1990 - Feature selection using a multilayer perceptron.pdf:pdf},
journal = {Journal of Neural Network Computing},
mendeley-groups = {Thesis/Used},
pages = {1--14},
title = {{Feature selection using a multilayer perceptron}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6617{\&}rep=rep1{\&}type=pdf},
year = {1990}
}
@article{Ruder2016,
abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
archivePrefix = {arXiv},
arxivId = {1604.08610},
author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1604.08610},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1604.08610v1.pdf:pdf},
pages = {1--14},
title = {{Artistic style transfer for videos}},
url = {http://arxiv.org/abs/1604.08610},
year = {2016}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/Unknown - 1986 - {\textcopyright} 1986 Nature Publishing Group.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {Thesis/Articles},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learning+representations+by+back-propagating+errors{\&}ots=zZDj2mGYVQ{\&}sig=mcyEACaE{\_}ZB4FB4xsoTgXgcbE2g$\backslash$nhttp://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learni},
volume = {323},
year = {1986}
}
@book{Rumelhart1988,
author = {Rumelhart, David E and McClelland, James L and Group, P D P Research and Others},
mendeley-groups = {Thesis/Used},
publisher = {IEEE},
title = {{Parallel distributed processing}},
volume = {1},
year = {1988}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.0575v3.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Samuel1959,
abstract = {Two machine-learning procedures have been investigated 1 in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Further- more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
author = {Samuel, Artur L},
doi = {10.1147/rd.33.0210},
file = {:Users/omegak/Documents/Papers/B, G-, Samuel - Unknown - Some Studies in Machine Learning.pdf:pdf},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of research and development},
mendeley-groups = {Thesis/Used},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
volume = {3},
year = {1959}
}
@article{Sherrington1906,
author = {Sherrington, C S},
doi = {10.1113/jphysiol.1906.sp001139},
file = {:Users/omegak/Documents/Papers/Unknown - 1904 - OBSERVATIONS ON THE SCRATCH-REFLEX IN THE SPINAL DOG. By C. S. SHERRINGTON. (27.pdf:pdf},
isbn = {0022-3751 (Print)},
issn = {0022-3751},
journal = {The Journal of physiology},
mendeley-groups = {Thesis/Used},
pages = {1--50},
pmid = {16992835},
title = {{Observations on the scratch-reflex in the spinal dog.}},
volume = {34},
year = {1906}
}
@article{Tang2010,
abstract = {A laser-triangulating range camera uses a laser plane to light an object. If the position of the laser relative to the camera as well as certrain properties of the camera is known, it is possible to calculate the coordinates for all points along the profile of the object. If either the object or the camera and laser has a known motion, it is possible to combine several measurements to get a three-dimensional view of the object. Camera calibration is the process of finding the properties of the camera and enough information about the setup so that the desired coordinates can be calcu- lated. Several methods for camera calibration exist, but this thesis proposes a new method that has the advantages that the objects needed are relatively inexpensive and that only objects in the laser plane need to be observed. Each part of the method is given a thorough description. Several mathematical derivations have also been added as appendices for completeness. The proposed method is tested using both synthetic and real data. The results show that the method is suitable even when high accuracy is needed. A few suggestions are also made about how the method can be improved further.},
author = {Tang, Zhiqiang},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/FULLTEXT01.pdf:pdf},
journal = {Electrical Engineering},
number = {1},
pages = {13--34},
title = {{Institutionen f{\"{o}}r systemteknik Department of Electrical Engineering}},
url = {http://www.vehicular.isy.liu.se/Publications/MSc/09_EX_4227_JL.pdf},
volume = {54},
year = {2010}
}
@article{Tenenbaum1997,
abstract = {We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor struc- ture. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMsj, which generalizes earlier work on factorial mixture models [7, 3]. Significant per- formance improvement on a benchmark speech dataset shows the benefits of our approach.},
author = {Tenenbaum, J B and Freeman, W T},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1290-separating-style-and-content.pdf:pdf},
journal = {Advances in neural information processing},
title = {{Separating style and content with bilinear models.}},
volume = {9},
year = {1997}
}
@misc{Thagard2008,
author = {Thagard, Paul},
booktitle = {The Stanford Encyclopedia of Philosophy},
editor = {Zalta, Edward N.},
mendeley-groups = {Thesis/Articles},
title = {{Cognitive Science}},
year = {2008}
}
@article{Thorpe1989,
abstract = {Page 1. 1 BIOLOGICAL CONSTRAINTS ON CONNECTIONIST MODELLING Simon J. Thorpe and Michel Imbert, Institut des Neurosciences, Departement des Neurosciences de la Vision, Universit{\'{e}} Pierre et Marie Curie, 9, Quai St. Bernard, 75005, Paris, FRANCE. ABSTRACT},
author = {Thorpe, Simon J. and Imbert, Michel},
doi = {10.1.1.96.6484},
file = {:Users/omegak/Documents/Papers/Thorpe - Unknown - Biological constraints on connectionist modeling.pdf:pdf},
journal = {Connectionism in perspective},
keywords = {vision},
mendeley-groups = {Thesis/Articles},
number = {August 2016},
pages = {1--36},
title = {{Biological constraints on connectionist modelling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.6484{\&}rep=rep1{\&}type=pdf},
year = {1989}
}
@article{Upchurch2016,
abstract = {We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4% lower dissimilarity to the ground truth than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1603.02003},
author = {Upchurch, Paul and Snavely, Noah and Bala, Kavita},
eprint = {1603.02003},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1603.02003v1.pdf:pdf},
title = {{From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators}},
url = {http://arxiv.org/abs/1603.02003},
year = {2016}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1506.06579v1.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.1556.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {ImageNet Challenge},
mendeley-groups = {Thesis,Used in thesis},
pages = {1--10},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Tenenbaum2000,
abstract = {Perceptual systems routinely separate "content" from "style," classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
author = {Tenenbaum, J B and Freeman, W T},
doi = {10.1162/089976600300015349},
file = {:Users/omegak/Desktop/10.1162@089976600300015349.pdf:pdf},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1247--1283},
pmid = {10935711},
title = {{Separating style and content with bilinear models}},
volume = {12},
year = {2000}
}
@article{Visin2015,
abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
archivePrefix = {arXiv},
arxivId = {1505.00393},
author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua},
eprint = {1505.00393},
file = {:Users/omegak/Documents/Papers/Visin et al. - 2015 - ReNet A Recurrent Neural Network Based Alternative to Convolutional Networks.pdf:pdf},
journal = {Arxiv},
mendeley-groups = {Thesis/Used},
pages = {1--9},
title = {{ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks}},
url = {http://arxiv.org/abs/1505.00393},
year = {2015}
}
@article{Werbos1974,
author = {Werbos, Paul},
mendeley-groups = {Thesis/Used},
title = {{Beyond regression: New tools for prediction and analysis in the behavioral sciences}},
year = {1974}
}
@article{Wilkniss1998,
author = {Wilkniss, Sandra and Davis, Kristin},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/p292-lin.pdf:pdf},
isbn = {026206197X},
issn = {1095-158X},
journal = {Psychiatric rehabilitation journal},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {244--8},
pmid = {20061264},
title = {{Book reviews.WordNet: An Electronic Lexical Database}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20061264},
volume = {33},
year = {1998}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
mendeley-groups = {Thesis},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{Zadeh1994,
author = {Zadeh, Lofti A.},
doi = {10.1145/175247.175255},
file = {:Users/omegak/Documents/Papers/Zadeh - 1994 - Fuzzy logic, neural networks, and soft computing.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
mendeley-groups = {Thesis/Used},
number = {3},
pages = {77--85},
title = {{Fuzzy logic, neural networks, and soft computing}},
volume = {37},
year = {1994}
}
@article{ZHANG1999,
abstract = {In this paper, we report our experiments on feature-based facial expression recognition within an architecture based on a two-layer perceptron. We investigate the use of two types of features extracted from face images:  the geometric positions of a set of fiducial points on a face, and a set of multi-scale and multi-orientation Gabor wavelet coefficients at these points.  They can be used either independently or jointly. The recognition performance with different types of features has been compared, which shows that Gabor wavelet coefficients are much more powerful than geometric positions.  Furthermore, since the first layer of the perceptron actually performs a nonlinear reduction of the dimensionality of the feature space, we have also studied the desired number of hidden units, i.e., the appropriate dimension to represent a facial expression in order to achieve a good recognition rate.   It turns out that five to seven hidden units are probably enough to represent the space of feature expressions. Then, we have investigated the importance of each individual fiducial point to facial expression recognition.  Sensitivity analysis reveals that points on cheeks and on forehead carry little useful information.  After discarding them, not only the computational efficiency increases, but also the generalization performance slightly improves. Finally, we have studied the significance of image scales. Experiments show that facial expression recognition is mainly a low frequency process, and a spatial resolution of 64 pixels  64 pixels is probably enough},
author = {ZHANG, ZHENGYOU},
doi = {10.1142/S0218001499000495},
file = {:Users/omegak/Documents/Papers/ZHANG - 1999 - Feature-Based Facial Expression Recognition Sensitivity Analysis and Experiments With a Multilayer Perceptron.pdf:pdf},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {facial expression recognition,gabor wavelets,image scale,learning,multilayer perceptron,sensitivity analysis,},
mendeley-groups = {Thesis,Thesis/Used},
number = {06},
pages = {893--911},
title = {{Feature-Based Facial Expression Recognition: Sensitivity Analysis and Experiments With a Multilayer Perceptron}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001499000495},
volume = {13},
year = {1999}
}


% -- Images --------------------------------------------------------------------

@image{AI4562013,
author = {AI456},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/AI456 - 2013 - Error surface of a linear neuron with two input weights.png:png},
mendeley-groups = {Thesis/Images},
title = {{Error surface of a linear neuron with two input weights}},
url = {https://upload.wikimedia.org/wikipedia/commons/6/6d/Error{\_}surface{\_}of{\_}a{\_}linear{\_}neuron{\_}with{\_}two{\_}input{\_}weights.png},
urldate = {21/08/2016},
year = {2013}
}
@image{Aphex342015,
abstract = {Input volume connected to a convolutional layer},
author = {Aphex34},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/Aphex34 - 2015 - Convolutional Layer.html:html},
mendeley-groups = {Thesis/Images},
title = {{Convolutional Layer}},
url = {https://en.wikipedia.org/wiki/File:Conv_layer.png},
urldate = {2016-08-04}
}
@image{Apple,
author = {Apple},
booktitle = {developer.apple.com},
file = {:Users/omegak/Documents/Papers/Apple - Unknown - Kernel Convolution.jpg:jpg},
mendeley-groups = {Thesis/Images},
title = {{Kernel Convolution}},
url = {https://developer.apple.com/library/prerelease/content/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html},
urldate = {08/08/2016}
}
@image{Goodspeed2015,
author = {Goodspeed, Elizabeth},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/Goodspeed - 2015 - Perceptron Training.png:png},
mendeley-groups = {Thesis/Images},
title = {{Perceptron Training}},
url = {https://en.wikipedia.org/wiki/File:Perceptron{\_}example.svg},
urldate = {2016-08-16},
year = {2015}
}
@image{Guerzhoy2016,
author = {Guerzhoy, Michael},
booktitle = {University of Toronto},
file = {:Users/omegak/Documents/Papers/Guerzhoy - 2016 - Modern ConvNet Architectures.pdf:pdf},
mendeley-groups = {Thesis/Images},
title = {{Modern ConvNet Architectures}},
url = {http://www.cs.toronto.edu/~guerzhoy/321/lec/W06/convnets.pdf},
urldate = {2016-08-06}
}
@image{Honorio2013,
author = {Honório, Vinícius Gonçalves and Maria, Maltarollo Káthia and da Silva, Albérico Borges Ferreira},
booktitle = {intechopen.com},
doi = {10.5772/51275},
file = {:Users/omegak/Documents/Papers/Honório, Maria, Silva - 2013 - Human Neuron vs Artificial Neuron.png:png},
mendeley-groups = {Thesis/Images},
title = {{Human Neuron vs Artificial Neuron}},
url = {http://www.intechopen.com/books/artificial-neural-networks-architectures-and-applications/applications-of-artificial-neural-networks-in-chemical-problems},
year = {2013}
}
@image{Karpathy,
author = {Karpathy, Andrej},
booktitle = {Stanford Computer Science Class},
file = {:Users/omegak/Documents/Papers/Karpathy - Unknown - General Pool.jpeg:jpeg},
mendeley-groups = {Thesis/Images},
title = {{General Pool}},
url = {http://cs231n.github.io/convolutional-networks/#pool},
urldate = {2016-08-07}
}
@image{Karpathya,
author = {Karpathy, Andrej},
booktitle = {Stanford Computer Science Class},
file = {:Users/omegak/Documents/Papers/Karpathy - Unknown - Maxpool.jpeg:jpeg},
mendeley-groups = {Thesis/Images},
title = {{Maxpool}},
url = {http://cs231n.github.io/convolutional-networks/#pool},
urldate = {2016-08-07}
}
@image{Medina2013,
author = {Medina, Gonzalo},
booktitle = {tex.stackexchange.com},
file = {:Users/omegak/Documents/Papers/Medina - 2013 - Neural Network in Tikz.html:html},
mendeley-groups = {Thesis/Images},
title = {{Neural Network in Tikz}},
url = {http://tex.stackexchange.com/a/132471},
urldate = {15/08/2016},
year = {2013}
}
@image{Medina2013A,
author = {Medina, Gonzalo},
booktitle = {tex.stackexchange.com},
file = {:Users/omegak/Documents/Papers/Medina - 2013 - Multilayer Perceptron in Tikz.html:html},
mendeley-groups = {Thesis/Images},
title = {{Multilayer Perceptron in Tikz}},
url = {http://tex.stackexchange.com/a/132471},
urldate = {2016-08-18},
year = {2013}
}
