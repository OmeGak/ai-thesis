@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:Users/omegak/Documents/Papers/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
journal = {None},
mendeley-groups = {Thesis,Thesis/Used},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Adelson1985,
abstract = {A motion sequence may be represented as a single pattern in x-y-t space; a velocity of motion corresponds to a three-dimensional orientation in this space. Motion sinformation can be extracted by a system that responds to the oriented spatiotemporal energy. We discuss a class of models for human motion mechanisms in which the first stage consists of linear filters that are oriented in space-time and tuned in spatial frequency. The outputs of quadrature pairs of such filters are squared and summed to give a measure of motion energy. These responses are then fed into an opponent stage. Energy models can be built from elements that are consistent with known physiology and psychophysics, and they permit a qualitative understanding of a variety of motion phenomena.},
author = {Adelson, E.H. and Bergen, J.R.},
doi = {10.1364/JOSAA.2.000284},
file = {:Users/omegak/Documents/Papers/Adelson, Bergen - 1985 - Spatiotemporal energy models for the perception of motion.pdf:pdf},
isbn = {0740-3232 (Print)},
issn = {1084-7529},
journal = {Journal of the Optical Society of America A, Optics and image science},
mendeley-groups = {Thesis/Articles},
number = {2},
pages = {284--299},
pmid = {3973762},
title = {{Spatiotemporal energy models for the perception of motion}},
url = {http://www.opticsinfobase.org/abstract.cfm?URI=josaa-2-2-284},
volume = {2},
year = {1985}
}
@article{Alonso2008,
author = {Alonso, Jose-Manuel and Chen, Yao},
doi = {doi:10.4249/scholarpedia.5393},
file = {:Users/omegak/Documents/Papers/Alonso, Chen - 2008 - Receptive Field.html:html},
journal = {Shcolarpedia},
mendeley-groups = {Thesis/Used},
number = {1},
pages = {5393},
title = {{Receptive Field}},
url = {http://www.scholarpedia.org/article/Receptive_field},
volume = {4},
year = {2008}
}
@article{Ashikhmin2003,
abstract = { The article presents an algorithm for texture transfer between images that is up to several orders of magnitude faster than current state-of-the-art techniques. I demonstrate how the technique can leverage self-similarity of complex images to increase resolution of some types of images and to create novel, artistic looking images from photographs without any prior artistic source. Compared to other alternatives, methods based on texture transfer are global in the sense that the user need not deal with details such as defining and painting individual brush strokes. Texture transfer methods are also more general since they don't need to emulate any particular artistic style (line drawing, hatching, realistic oil painting, and so on). Not surprisingly, there is a price to pay for this generality - an algorithm designed for a specific artistic style will most likely produce results superior to those presented in the paper for that particular case.},
author = {Ashikhmin, Michael},
doi = {10.1109/MCG.2003.1210863},
file = {:Users/omegak/Documents/Papers/Ashikhmin - 2003 - Fast Texture Transfer.pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
number = {4},
pages = {38--42},
title = {{Fast texture transfer}},
volume = {23},
year = {2003}
}
@article{Bahrampour2015,
abstract = {Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of five deep learning frameworks, namely Caffe, Neon, TensorFlow, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Caffe is the easiest for evaluating the performance of standard deep architectures. Finally, TensorFlow is a very flexible framework, similar to Theano, but its performance is currently not competitive compared to the other studied frameworks.},
archivePrefix = {arXiv},
arxivId = {1511.06435},
author = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
doi = {10.1227/01.NEU.0000297044.82035.57},
eprint = {1511.06435},
file = {:Users/omegak/Documents/Papers/Bahrampour et al. - Unknown - Comparative Study of Deep Learning Software Frameworks.pdf:pdf},
isbn = {9150617397},
issn = {19494912},
journal = {Journal of Marketing and Operations Management Research},
keywords = {7000:Marketing,8331:Internet services industry,9130:Experiment/theoretical treatment,9175:Western Europe,9190:United States,Business And Economics--Marketing And Purchasing,Comparative studies,Discount coupons,Europe,Lotteries,Older people,Social networks,United States--US,Viral marketing},
mendeley-groups = {Thesis/Articles},
number = {1},
pages = {1--14},
title = {{Comparative Study of Deep Learning Software Frameworks}},
url = {http://search.proquest.com/docview/1626785137?accountid=10755$\backslash$nhttp://sfx.bib-bvb.de/sfx{\_}uben?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=unknown{\&}sid=ProQ:ProQ:abiglobal{\&}atitle=BEHAVIOUR+OF+ELDERLY+USERS+ON+FACEBOOK+TOWARD+VIRAL+MA$\backslash$n},
volume = {2},
year = {2015}
}
@article{Baker2009,
abstract = {This article is the second part of an updated version of the "MINDS 2006-2007 Report of the Speech Understanding Working Group," one of five reports emanating from two workshops entitled "Meeting of the MINDS: Future Directions for Human Language Technology," sponsored by the U.S. Disruptive Technology Office (DTO). (MINDS is an acronym for "machine translation, information retrieval, natural-language processing, data resources, and speech understanding").},
author = {Baker, Janet M. and Deng, Li and Khudanpur, Sanjeev and Lee, Chin Hui and Glass, James R. and Morgan, Nelson and O'Shaughnessy, Douglas},
doi = {10.1109/MSP.2009.932707},
file = {:Users/omegak/Documents/Papers/Baker et al. - 2009 - Updated MINDS report on speech recognition and understanding.pdf:pdf},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
mendeley-groups = {Thesis/Articles},
number = {4},
pages = {78--85},
pmid = {19055118},
title = {{Updated MINDS report on speech recognition and understanding}},
volume = {26},
year = {2009}
}
@article{Bergstra2010,
abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy's syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy's syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
author = {Bergstra, James and Breuleux, Olivier and Bastien, Frederic Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
file = {:Users/omegak/Documents/Papers/Bergstra et al. - 2010 - Theano a CPU and GPU math compiler in Python.pdf:pdf},
journal = {Proceedings of the Python for Scientific Computing Conference (SciPy)},
mendeley-groups = {Thesis,Thesis/Used},
number = {Scipy},
pages = {1--7},
title = {{Theano: a CPU and GPU math compiler in Python}},
url = {http://www-etud.iro.umontreal.ca/~wardefar/publications/theano_scipy2010.pdf},
year = {2010}
}
@misc{Bernacki2005,
author = {Bernacki, Mariusz and W{\l}odarczyk, Przemys{\l}aw},
booktitle = {Katedra Elektroniki AGH},
file = {:Users/omegak/Documents/Papers/Bernacki, W{\l}odarczyk - 2005 - Backpropagation.html:html},
mendeley-groups = {Thesis/Articles},
title = {{Backpropagation}},
url = {http://home.agh.edu.pl/{~}vlsi/AI/backp{\_}t{\_}en/backprop.html},
urldate = {2016-08-21},
year = {2005}
}
@article{Boureau2010,
abstract = {Many modern visual recognition algorithms incorporate a step of spatial ‘pooling', where the outputs of several nearby feature detectors are combined into a local or global ‘bag of features', in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly influence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.},
author = {Boureau, Y-Lan and Ponce, Jean and LeCun, Yann},
doi = {citeulike-article-id:8496352},
file = {:Users/omegak/Documents/Papers/Boureau - 2009 - A Theoretical Analysis of Feature Pooling in Visual Recognition.pdf:pdf},
journal = {Icml},
mendeley-groups = {Thesis/Articles},
pages = {111--118},
title = {{A Theoretical Analysis of Feature Pooling in Visual Recognition}},
url = {http://www.ece.duke.edu/{~}lcarin/icml2010b.pdf},
year = {2010}
}
@article{Cheng2015,
abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
archivePrefix = {arXiv},
arxivId = {1511.04587},
author = {Cheng, Zezhou},
doi = {10.1109/ICCV.2015.55},
eprint = {1511.04587},
file = {:Users/omegak/Documents/Papers/Cheng - Unknown - Deep Colorization.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {0162-8828},
journal = {Cvpr},
mendeley-groups = {Thesis/Articles},
pages = {415--423},
title = {{Deep Colorization}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Cheng{\_}Deep{\_}Colorization{\_}ICCV{\_}2015{\_}paper.pdf},
volume = {1},
year = {2015}
}
@article{Collobert2002,
abstract = {Many scientific communities have expressed a growing interest in machine learning algorithms recently, mainly due to the generally good results they provide, compared to traditional statistical or AI approaches. However, these machine learning algorithms are often complex to implement and to use properly and efficiently. We thus present in this paper a new machine learning software library in which most state-of-the-art algorithms have already been implemented and are available in a unified framework, in order for scientists to be able to use them, compare them, and even extend them. More interestingly, this library is freely available under a BSD license and can be retrieved on the web by everyone.},
author = {Collobert, Ronan and Bengio, Samy and Mariethoz, Johnny},
file = {:Users/omegak/Documents/Papers/Collobert, Bengio, Mariethoz - 2002 - Torch A Modular Machine Learning Software Library.pdf:pdf},
isbn = {IDIAP-RR 02-46},
mendeley-groups = {Thesis,Thesis/Used},
pages = {7},
title = {{Torch: A Modular Machine Learning Software Library}},
year = {2002}
}
@article{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm},
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
file = {:Users/omegak/Documents/Papers/Dean et al. - Unknown - Large Scale Distributed Deep Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {NIPS 2012: Neural Information Processing Systems},
mendeley-groups = {Thesis/Articles},
pages = {1--11},
pmid = {43479959},
title = {{Large Scale Distributed Deep Networks}},
year = {2012}
}
@article{Chen2012,
abstract = {The Context-Dependent Deep-Neural-Network HMM, or CD- DNN-HMM, is a recently proposed acoustic-modeling tech- nique for HMM-based speech recognition that can greatly out- perform conventional Gaussian-mixture based HMMs. For ex- ample, a CD-DNN-HMM trained on the 2000h Fisher corpus achieves 14.4{\%} word error rate on the Hub5'00-FSH speaker- independent phone-call transcription task, compared to 19.6{\%} obtained by a state-of-the-art, conventional discriminatively trained GMM-based HMM. That CD-DNN-HMM, however, took 59 days to train on a modern GPGPU—the immense computational cost of the mini- batch based back-propagation (BP) training is a major road- block. Unlike the familiar Baum-Welch training for conven- tional HMMs, BP cannot be efficiently parallelized across data. In this paper we show that the pipelined approximation to BP, which parallelizes computation with respect to layers, is an efficient way of utilizing multiple GPGPU cards in a single server. Using 2 and 4 GPGPUs, we achieve a 1.9 and 3.3 times end-to-end speed-up, at parallelization efficiency of 0.95 and 0.82, respectively, at no loss of recognition accuracy.},
author = {Chen, Xie and Eversole, Adam and Li, Gang and Yu, Dong and Seide, Frank},
file = {:Users/omegak/Documents/Papers/Chen et al. - Unknown - Pipelined Back-Propagation for Context-Dependent Deep Neural Networks.pdf:pdf},
isbn = {9781622767595},
journal = {Proc. Interspeech},
mendeley-groups = {Thesis/Articles},
pages = {2--5},
title = {{Pipelined back-propagation for context-dependent deep neural networks}},
url = {http://research.microsoft.com/pubs/173312/DNN-Pipeline-Interspeech2012.pdf},
year = {2012}
}
@article{Decaudin1996,
author = {Decaudin, Philippe},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/decaudin_1996.pdf:pdf},
journal = {Syntim Project Inria},
keywords = {cartoon,cel shading,computer graphics,non-photorealistic rendering},
mendeley-groups = {Thesis,Used in thesis},
number = {June},
pages = {1--11},
title = {{Cartoon-looking rendering of 3D-scenes}},
url = {ftp://meria.idc.ac.il/Faculty/arik/LODSeminar/07Shading/decaudin_1996.pdf},
year = {1996}
}
@article{Deng1999,
abstract = {Major speech production models from speech science literature and a number of popular statistical " generative " models of speech used in speech technology are surveyed. Strengths and weaknesses of these two styles of speech models are analyzed, pointing to the need to integrate the respective strengths while eliminating the respective weaknesses. As an example, a statistical task-dynamic model of speech production is described, motivated by the original deterministic version of the model and targeted for integrated-multilingual speech recognition applications. Methods for model parameter learning (training) and for likelihood computation (recognition) are described based on statistical optimization princi-ples integrated in neural network and dynamic system theories.},
author = {Deng, Li},
file = {:Users/omegak/Documents/Papers/Deng - 1999 - Computational models for speech production.pdf:pdf},
journal = {Computational models of speech pattern processing},
mendeley-groups = {Thesis/Articles},
pages = {199--213},
title = {{Computational models for speech production}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-60087-6{\_}20},
year = {1999}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/imagenet_cvpr09.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Thesis,Used in thesis},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Deng2014,
abstract = {Deep learning, Machine learning, Artificial intelligence, Neural networks, Deep neural networks, Deep stacking networks, Autoencoders, Supervised learning, Unsupervised learning, Hybrid deep networks, Object recognition, Computer vision, Natural language processing, Language models, Multi-task learning, Multi-modal processing},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
eprint = {1309.1501},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1601988141 [Deng & Yu] Deep Learning - Methods and Applications.2014@CR.pdf:pdf},
isbn = {1601988141},
issn = {09598138},
number = {2013},
pages = {206},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
url = {http://books.google.com/books?id=46qNoAEACAAJ&pgis=1},
volume = {7},
year = {2014}
}
@article{Dosovitskiy2015,
abstract = {Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features when combined with a strong prior. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.},
archivePrefix = {arXiv},
arxivId = {1506.02753},
author = {Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-10593-2_13},
eprint = {1506.02753},
file = {:Users/omegak/Documents/Papers/Brox - Unknown - Inverting Visual Representations with Convolutional Networks.pdf:pdf},
isbn = {9781941643327},
journal = {arXiv preprint arXiv:1506.02753},
mendeley-groups = {Thesis/Articles},
pages = {1--15},
title = {{Inverting Visual Representations with Convolutional Networks}},
url = {http://arxiv.org/abs/1506.02753},
year = {2015}
}
@article{Eigen2013,
abstract = {Photographs taken through a window are often compromised by dirt or $\backslash$nrain present on the window surface. Common cases of this include pictures taken $\backslash$nfrom inside a vehicle, or outdoor security cameras mounted inside a protective $\backslash$nenclosure. At capture time, defocus can be used to remove the artifacts, but $\backslash$nthis relies on achieving a shallow depth-of-field and placement of the camera $\backslash$nclose to the window. Instead, we present a post-capture image processing $\backslash$nsolution that can remove localized rain and dirt artifacts from a single image. $\backslash$nWe collect a dataset of clean/corrupted image pairs which are then used to train $\backslash$na specialized form of convolutional neural network. This learns how to map $\backslash$ncorrupted image patches to clean ones, implicitly capturing the characteristic $\backslash$nappearance of dirt and water droplets in natural images. Our models demonstrate $\backslash$neffective removal of dirt and rain in outdoor test conditions.},
author = {Eigen, David and Krishnan, Dilip and Fergus, Rob},
doi = {10.1109/ICCV.2013.84},
file = {:Users/omegak/Documents/Papers/Eigen, Krishnan, Fergus - Unknown - Restoring An Image Taken Through a Window Covered with Dirt or Rain.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Thesis/Articles},
pages = {633--640},
title = {{Restoring an image taken through a window covered with dirt or rain}},
url = {http://www.cs.nyu.edu/{~}deigen/rain/restore-iccv13.pdf},
year = {2013}
}
@article{Elgammal2004,
abstract = {Bilinear and multi-linear models have been successful in decomposing static image ensembles into perceptually orthogonal sources of variations, e.g., separation of style and content. If we consider the appearance of human motion such as gait, facial expression and gesturing, most of such activities result in nonlinear manifolds in the image space. The question that we address in this paper is how to separate style and content on manifolds representing dynamic objects. In this paper we learn a decomposable generative model that explicitly decomposes the intrinsic body configuration (content) as a function of time from the appearance (style) of the person performing the action as time-invariant parameter. The framework we present in this paper is based on decomposing the style parameters in the space of nonlinear functions which map between a learned unified nonlinear embedding of multiple content manifolds and the visual input space.},
author = {Elgammal, Ahmed and Lee, Chan-su},
doi = {10.1109/CVPR.2004.1315070},
file = {:Users/omegak/Documents/Papers/Elgammal, Lee - 2004 - Separating Style and Content on a Nonlinear Manifold.pdf:pdf},
isbn = {0769521584},
issn = {10636919},
journal = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2004 CVPR 2004},
number = {C},
pages = {478--485},
title = {{Separating style and content on a nonlinear manifold}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315070},
volume = {00},
year = {2004}
}
@misc{Esaak,
author = {Esaak, Shelley},
title = {{What is the Definition of Art?}},
url = {http://arthistory.about.com/cs/reference/f/what_is_art.htm},
urldate = {2016-07-14}
}
@article{Farley1954,
author = {Farley, BWAC and Clark, W},
journal = {Transactions of the IRE Professional Group on Information Theory},
mendeley-groups = {Thesis/Articles},
number = {4},
pages = {76--84},
title = {{Simulation of self-organizing systems by digital computer}},
volume = {4},
year = {1954}
}
@article{Fei-Fei2007,
abstract = {Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
doi = {10.1016/j.cviu.2005.09.012},
file = {:Users/omegak/Documents/Papers/Fei-fei, Fergus, Perona - 2007 - Learning generative visual models from few training examples An incremental Bayesian approach tested o.pdf:pdf},
isbn = {1077-3142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Bayesian model,Categorization,Generative model,Incremental learning,Object recognition},
mendeley-groups = {Thesis/Articles},
number = {1},
pages = {59--70},
title = {{Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories}},
volume = {106},
year = {2007}
}
@article{Freund1999,
abstract = {We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithmwith Helmbold andWarmuth's leave-one-outmethod. LikeVapnik's maximal margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximalmargin classifiers on the same problem, while saving significantly on computation time and programming effort},
author = {Freund, Yoav and Schapire, Robert E.},
doi = {10.1023/A:1007662407062},
file = {:Users/omegak/Documents/Papers/Freund, Schapire - 1999 - Large Margin Classification Using the Perceptron Algorithm.pdf:pdf},
isbn = {1581130570},
issn = {08856125},
journal = {Machine Learning},
mendeley-groups = {Thesis/Articles},
number = {3},
pages = {277--296},
title = {{Large margin classification using the perceptron algorithm}},
url = {http://cseweb.ucsd.edu/{~}yfreund/papers/LargeMarginsUsingPerceptron.pdf},
volume = {37},
year = {1999}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:Users/omegak/Documents/Papers/Fukushima - 1980 - Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in pos.pdf:pdf},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
mendeley-groups = {Thesis/Used},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Gatys2015A,
abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1505.07376},
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
eprint = {1505.07376},
file = {:Users/omegak/Documents/Papers/Gatys, Ecker, Bethge - 2015 - Texture Synthesis Using Convolutional Neural Networks.pdf:pdf},
issn = {10495258},
journal = {Nips},
mendeley-groups = {Thesis/Articles},
pages = {1--10},
title = {{Texture Synthesis Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1505.07376},
year = {2015}
}
@article{Gatys2015B,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/1508.06576v2.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {neural algorithm of artistic,style},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Griffin2007,
abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 1 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) aftifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, the benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching 2 algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
author = {Griffin, G and Holub, a and Perona, P},
file = {:Users/omegak/Documents/Papers/Unknown - Unknown - No Title.pdf:pdf},
isbn = {UCB/CSD-04-1366},
journal = {Caltech mimeo},
mendeley-groups = {Thesis/Articles},
number = {1},
pages = {20},
title = {{Caltech-256 object category dataset}},
url = {http://authors.library.caltech.edu/7694},
volume = {11},
year = {2007}
}
@incollection{Hairer2008,
abstract = {Undergraduate students often struggle to learn mathematics because introductory classes are taught in large lectures that do not engage students in active problem-solving. These students do not connect mathematics to their lives and feel that learning mathematics is a solitary undertaking. We now use Tablet PCs in a networked classroom to address these challenges. Students in classes that use the Tablet PCs can view and annotate the instructor's Powerpoint slides in real time and also participate in interactive problem-solving. Students save their own annotated slides for subsequent review. They also have immediate access to the synchronized screen capture and audio recording of the class since the instructor posts this file to the course management website. These technological interventions allow students to focus on classroom activities rather than on note-taking. To date, students have taken three introductory undergraduate mathematics courses (College Algebra and Trigonometry, Calculus I, and Calculus II) using Tablet PCs. Student attendance and retention were better in the cohort of students who participated in the Tablet PC courses than in comparable non-Tablet PC courses taught by the same instructor. The evaluation of the instructor was unchanged.},
author = {Hairer, E. and Wanner, G.},
booktitle = {Undergraduate Texts in Mathematics},
chapter = {Calculus i},
doi = {10.1007/978-0-387-92712-1},
file = {:Users/omegak/Documents/Papers/Hairer, Wanner - 2008 - Analysis by its History.pdf:pdf},
isbn = {3540901639},
issn = {0172-6056},
mendeley-groups = {Thesis/Articles},
pages = {389},
publisher = {Springer Science {\&} Business Media},
title = {{Analysis by its History}},
url = {http://www.amazon.com/dp/1441970223},
volume = {51},
year = {2008}
}
@article{Heeger1995,
abstract = {This paper describes a method for synthesizing images that match the texture appearanceof a given digitized sample. This synthesis is completely automatic and requires only the “target” texture as input. It allows generation of as much texture as desired so that any object can be covered. It can be used to produce solid textures for creating textured 3-d objects without the distortions inherent in texture mapping. It can also be used to synthesize texture mixtures, images that look a bit like each of several digitized samples. The approach is based on a model of human texture perception, and has potential to be a practically useful tool for graphics applications.},
author = {Heeger, David J and Bergen, James R},
doi = {10.1109/ICIP.1995.537718},
file = {:Users/omegak/Documents/Papers/Heeger - 1995 - Pyramid-Based Texture AnalysisSynthesis.pdf:pdf},
isbn = {0-7803-3122-2},
journal = {Proceedings., International Conference on Image Processing},
mendeley-groups = {Thesis/Articles},
pages = {10},
title = {{Pyramid-based texture analysis/synthesis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=537718},
year = {1995}
}
@article{Hertzmann2001,
abstract = {This paper describes a new framework for processing images by example, called image analogies. The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a filtered version of the other, is presented as training data; and an application phase, in which the learned filter is applied to some new target image in order to create an analogous filtered result. Image analogies are based on a simple multi-scale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of image filter effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are texturized with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.},
author = {Hertzmann, Aaron and Jacobs, Charles E and Oliver, Nuria and Curless, Brian and Salesin, David H},
doi = {10.1145/383259.383295},
file = {:Users/omegak/Documents/Papers/Hertzmann et al. - 2001 - Image analogies.pdf:pdf},
isbn = {158113374X},
journal = {Proceedings of the 28th annual conference on Computer graphics and interactive techniques SIGGRAPH 01},
number = {August},
pages = {327--340},
title = {{Image analogies}},
url = {http://portal.acm.org/citation.cfm?doid=383259.383295},
volume = {2001},
year = {2001}
}
@article{Hinton1990,
author = {Hinton, Geoffrey I},
file = {:Users/omegak/Documents/Papers/Hinton - Unknown - Connectionist Learning Procedures.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Machine Learning -- an Artificial Intelligence Approach},
mendeley-groups = {Thesis/Articles},
number = {1989},
pages = {555--610},
title = {{Connectionist Learning Procedures}},
volume = {III},
year = {1990}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:Users/omegak/Documents/Papers/Hinton - 2006 - Communicated by Yann Le Cun A Fast Learning Algorithm for Deep Belief Nets 500 units 500 units.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
mendeley-groups = {Thesis/Articles},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@book{Horn2012,
author = {Horn, Roger A and Johnson, Charles R},
mendeley-groups = {Thesis/Articles},
pages = {441},
publisher = {Cambridge university press},
title = {{Matrix analysis}},
year = {2012}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
mendeley-groups = {Thesis/Used},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Hubel1968,
abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded. 2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent. 3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other. 4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven. 5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
author = {Hubel, D. H. and Wiesel, T N},
doi = {papers://47831562-1F78-4B52-B52E-78BF7F97A700/Paper/p352},
file = {:Users/omegak/Documents/Papers/Hubel, Wiesel - 1968 - Receptive Fields and Functional Architecture of monkey striate cortex.pdf:pdf},
isbn = {0022-3751 (Print) 0022-3751 (Linking)},
issn = {0022-3751},
journal = {Journal of Physiology},
keywords = {classics,macaque (rhesus) monkey,visual cortex,visual system},
mendeley-groups = {Thesis,Thesis/Used},
pages = {215--243},
pmid = {4966457},
title = {{Receptive Fields and Functional Architecture of monkey striate cortex}},
volume = {195},
year = {1968}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor and Eecs, U C Berkeley},
eprint = {arXiv:1408.5093v1},
file = {:Users/omegak/Documents/Papers/Jia et al. - 2014 - Convolutional_Architecture_Feature_Embedding.pdf:pdf},
isbn = {9781450330633},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
mendeley-groups = {Thesis,Thesis/Used},
title = {{Convolutional Architecture Feature Embedding}},
year = {2014}
}
@misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
}
@article{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph{\{}per-pixel{\}} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph{\{}perceptual{\}} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155},
file = {:Users/omegak/Documents/Papers/Johnson, Alahi, Fei-Fei - 2016 - Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:pdf},
journal = {Arxiv},
keywords = {deep learning,style transfer,super-resolution},
mendeley-groups = {Thesis/Articles},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {http://arxiv.org/abs/1603.08155},
year = {2016}
}
@article{Julesz1962,
abstract = {ual discrimination experiments were conducted using unfamiliar displays generated by a digital computer. The displays contained two side-by-side fields with different statistical, topological or heuristic properties. Discrimination was defined as that spontaneous visual process which gives the immediate impres- sion of two distinct fields. The condition for such discrimination was found to be based primarily on clusters or lines formed by proximate points of uniform brightness. A similar rule of con- nectivity with hue replacing brightness was obtained by using varicolored dots of equal subjective brightness. The limitations in discriminating complex line structures were also investigated.},
author = {Julesz, Bela},
doi = {10.1007/BF00344749},
file = {:Users/omegak/Documents/Papers/Nxactions - 1961 - Visual Pattern Discrimination.pdf:pdf},
issn = {0096-1000},
journal = {Information Theory, IRE Transactions on},
mendeley-groups = {Thesis/Articles},
pages = {41--46},
title = {{Visual pattern discrimination}},
url = {http://dl.acm.org/citation.cfm?id=1972$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1057698},
volume = {49},
year = {1962}
}
@article{Karayev2014,
abstract = {The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best -- even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.},
archivePrefix = {arXiv},
arxivId = {1311.3715},
author = {Karayev, Sergey and Trentacoste, Matthew and Han, Helen and Agarwala, Aseem and Darrell, Trevor and Hertzmann, Aaron and Winnemoeller, Holger},
doi = {http://dx.doi.org/10.5244/C.28.122},
eprint = {1311.3715},
file = {:Users/omegak/Documents/Papers/Han - Unknown - arXiv 1311 . 3715v3 cs . CV 23 Jul 2014 Recognizing Image Style.pdf:pdf},
journal = {Eccv},
mendeley-groups = {Thesis/Articles},
pages = {1--20},
title = {{Recognizing Image Style}},
url = {http://arxiv.org/abs/1311.3715},
year = {2014}
}
@article{Kim2015,
abstract = {We present a highly accurate single-image super-resolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification $\backslash$cite{\{}simonyan2015very{\}}. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates ({\$}10{\^{}}4{\$} times higher than SRCNN $\backslash$cite{\{}dong2015image{\}}) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.},
archivePrefix = {arXiv},
arxivId = {1511.04587},
author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
doi = {10.1109/TPAMI.2015.2439281},
eprint = {1511.04587},
file = {:Users/omegak/Documents/Papers/Kim, Lee, Lee - Unknown - Accurate Image Super-Resolution Using Very Deep Convolutional Networks.pdf:pdf},
issn = {0162-8828},
journal = {arXiv:1511.04587 [cs]},
mendeley-groups = {Thesis/Articles},
title = {{Accurate Image Super-Resolution Using Very Deep Convolutional Networks}},
url = {http://arxiv.org/abs/1511.04587$\backslash$nhttp://www.arxiv.org/pdf/1511.04587.pdf},
year = {2015}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
mendeley-groups = {Thesis/Articles},
pages = {1--60},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{\#}0},
year = {2009}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffret E},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Kyprianidis2013,
abstract = {This paper surveys the field of non-photorealistic rendering (NPR), focusing on techniques for transforming 2D input (images and video) into artistically stylized renderings. We first present a taxonomy of the 2D NPR algorithms developed over the past two decades, structured according to the design characteristics and behavior of each technique. We then describe a chronology of development from the semi-automatic paint systems of the early nineties, through to the automated painterly rendering systems of the late nineties driven by image gradient analysis. Two complementary trends in the NPR literature are then addressed, with reference to our taxonomy. First, the fusion of higher level computer vision and NPR, illustrating the trends toward scene analysis to drive artistic abstraction and diversity of style. Second, the evolution of local processing approaches toward edge-aware filtering for real-time stylization of images and video. The survey then concludes with a discussion of open challenges for 2D NPR identified in recent NPR symposia, including topics such as user and aesthetic evaluation.},
author = {Kyprianidis, Jan Eric and Collomosse, John and Wang, Tinghuai and Isenberg, Tobias},
doi = {10.1109/TVCG.2012.160},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/kyprianidis2013.pdf:pdf},
isbn = {10.1109/TVCG.2012.160},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Image and video stylization,artistic rendering,nonphotorealistic rendering (NPR)},
mendeley-groups = {Thesis,Used in thesis},
number = {5},
pages = {866--885},
pmid = {22802120},
title = {{State of the 'Art: A taxonomy of artistic stylization techniques for images and video}},
volume = {19},
year = {2013}
}
@misc{Lisa2010,
author = {{Laboratoire d'Informatique des Syst{\`{e}}mes Adaptatifs}},
booktitle = {deeplearning.net},
file = {:Users/omegak/Documents/Papers/Laboratoire d'Informatique des Syst{\`{e}}mes Adaptatifs - 2010 - LeNet Architecture.png:png},
mendeley-groups = {Thesis/Images},
title = {{LeNet Architecture}},
url = {http://deeplearning.net/tutorial/lenet.html},
urldate = {2016-08-08},
year = {2010}
}
@article{Lawrence1997,
abstract = {We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.},
author = {Lawrence, S and Giles, C.L. and {Ah Chung Tsoi} and Back, A.D.},
doi = {10.1109/72.554195},
file = {:Users/omegak/Documents/Papers/Lawrence et al. - 1997 - Face recognition a convolutional neural-network approach.pdf:pdf},
isbn = {1045-9227 VO - 8},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
mendeley-groups = {Thesis,Thesis/Used},
number = {1},
pages = {98--113},
pmid = {18255614},
title = {{Face recognition: a convolutional neural-network approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=554195},
volume = {8},
year = {1997}
}
@article{LeCun1995,
abstract = {The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classifier then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classifiers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with "raw" inputs (e.g. normalized images), and to rely on backpropagation to turn the first few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {LeCun, Y and Bengio, Y},
doi = {10.1109/IJCNN.2004.1381049},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/LeCun, Bengio - 1995 - Convolutional networks for images, speech, and time series.pdf:pdf},
isbn = {0262511029},
issn = {1098-7576},
journal = {The handbook of brain theory and neural networks},
mendeley-groups = {Thesis,Thesis/Used},
number = {April 2016},
pages = {255--258},
pmid = {17001990},
title = {{Convolutional networks for images, speech, and time series}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9297&rep=rep1&type=pdf},
volume = {3361},
year = {1995}
}
@article{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/omegak/Documents/Papers/LeCun et al. - 1998 - Gradient Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {convo-,document recogni-,fi-,gradient-based learning,graph transformer networks,lutional neural networks,machine learning,neural networks,nite state transducers,ocr,tion},
mendeley-groups = {Thesis,Thesis/Used},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@article{LeCun2004,
author = {{LeCun}, Leon Bottou Yann and Bottou, Leon},
journal = {Advances in neural information processing systems},
mendeley-groups = {Thesis/Articles},
pages = {217},
publisher = {The MIT Press},
title = {{Large scale online learning}},
volume = {16},
year = {2004}
}
@article{LeCun2004B,
author = {LeCun, Y. and Huang, Fu Jie and {L. Bottou}},
file = {:Users/omegak/Documents/Papers/LeCun, Huang, L. Bottou - 2004 - Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting. I.pdf:pdf},
mendeley-groups = {Thesis/Articles},
title = {{Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting}},
year = {2004}
}
@article{Lee2010,
author = {Lee, H. and Seo, S. and Ryoo, S. and Yoon, K.},
doi = {10.1145/1809939.1809945},
file = {:Users/omegak/Documents/Papers/Lee - 2010 - Directional texture transfer.pdf:pdf},
isbn = {9781450301244},
journal = {Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering},
keywords = {difference of direction,direction,dod,example-based rendering,non-photorealistic rendering,npr,texture transfer},
number = {212},
pages = {43--48},
title = {{Directional texture transfer}},
url = {http://portal.acm.org/citation.cfm?id=1809945},
volume = {1},
year = {2010}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/lin2011.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
mendeley-groups = {Thesis,Used in thesis},
number = {4},
pages = {297--312},
publisher = {Elsevier Inc.},
title = {{Perceptual visual quality metrics: A survey}},
url = {http://dx.doi.org/10.1016/j.jvcir.2011.01.005},
volume = {22},
year = {2011}
}
@article{Linnainmaa1976,
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
file = {:Users/omegak/Documents/Papers/Linnainmaa - 1976 - TAYLOR EXPANSION OF THE ACCUMULATED.pdf:pdf},
issn = {00063835},
journal = {Bit},
mendeley-groups = {Thesis/Articles},
number = {2},
pages = {146--160},
title = {{Taylor expansion of the accumulated rounding error}},
volume = {16},
year = {1976}
}
@article{Lo2015,
author = {Lo, Henry Z and Ding, Wei},
doi = {10.1109/ICDMW.2015.227},
file = {:Users/omegak/Documents/Papers/Lo, Ding - 2015 - Understanding Deep Networks with Gradients.pdf:pdf},
isbn = {9781467384926},
journal = {Neural Networks},
mendeley-groups = {Thesis/Articles},
pages = {5},
title = {{Understanding Deep Networks with Gradients}},
volume = {12},
year = {2015}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/omegak/Documents/Papers/Long, Shelhamer, Darrell - 2015 - Fully convolutional networks for semantic segmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Thesis/Articles},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Mahendran2014,
abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
archivePrefix = {arXiv},
arxivId = {1412.0035},
author = {Mahendran, Aravindh and Vedaldi, Andrea},
doi = {10.1109/CVPR.2015.7299155},
eprint = {1412.0035},
file = {:Users/omegak/Documents/Papers/Mahendran, Vedaldi - Unknown - Understanding Deep Image Representations by Inverting Them.pdf:pdf},
isbn = {9781467369640},
issn = {9781467369640},
mendeley-groups = {Thesis/Articles},
title = {{Understanding Deep Image Representations by Inverting Them}},
url = {http://arxiv.org/abs/1412.0035},
year = {2014}
}
@article{Matan1992,
abstract = {ocr-word.$\backslash$nIntegrated segmentation and recognition of connected$\backslash$nhandwritten characters with self-organizing nets.},
author = {Matan, O and Burges, J C and LeCun, Y and Denker, J S},
file = {:Users/omegak/Documents/Papers/Matan et al. - Unknown - Multi-Digit Recognition Using A Space Displacement Neural Network.pdf:pdf},
journal = {Proc. NIPS},
mendeley-groups = {Thesis/Articles},
pages = {488--495},
title = {{Multi-digit recognition using a space displacement neural network}},
url = {https://pdfs.semanticscholar.org/464e/8d981df7f326c3af6e9d7bd627f83e438816.pdf},
year = {1992}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/McCulloch, Pitts - 1943 - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
mendeley-groups = {Thesis/Articles},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Minsky1969,
author = {Minsky, Marvin and Papert, Seymour},
mendeley-groups = {Thesis/Used},
publisher = {MIT press},
title = {{Perceptrons}},
year = {1969}
}
@article{Molin2015,
abstract = {Pedestrian detection is an important field with applications in active safety systems$\backslash$r$\backslash$nfor cars as well as autonomous driving. Since autonomous driving and active$\backslash$r$\backslash$nsafety are becoming technically feasible now the interest for these applications$\backslash$r$\backslash$nhas dramatically increased.$\backslash$r$\backslash$nThe aim of this thesis is to investigate convolutional neural networks (CNN)$\backslash$r$\backslash$nfor pedestrian detection. The reason for this is that CNN have recently been$\backslash$r$\backslash$nsuccessfully applied to several different computer vision problems. The main$\backslash$r$\backslash$napplications of pedestrian detection are in real time systems. For this reason,$\backslash$r$\backslash$nthis thesis investigates strategies for reducing the computational complexity of$\backslash$r$\backslash$nforward propagation for CNN.$\backslash$r$\backslash$nThe approach used in this thesis for extracting pedestrians is to use a CNN to$\backslash$r$\backslash$nfind a probability map of where pedestrians are located. From this probability$\backslash$r$\backslash$nmap bounding boxes for pedestrians are generated.$\backslash$r$\backslash$nA method for handling scale invariance for the objects of interest has also$\backslash$r$\backslash$nbeen developed in this thesis. Experiments show that using this method gives$\backslash$r$\backslash$nsignificantly better results for the problem of pedestrian detection.$\backslash$r$\backslash$nThe accuracy which this thesis has managed to achieve is similar to the accuracy$\backslash$r$\backslash$nfor some other works which use CNN.},
author = {Molin, David},
file = {:Users/omegak/Documents/Papers/Molin - 2015 - Pedestrian detection using convolutional neural networks.pdf:pdf},
journal = {Electrical Engineering},
mendeley-groups = {Thesis/Articles},
number = {1},
pages = {13--34},
title = {{Pedestrian detection using convolutional neural networks}},
url = {http://liu.diva-portal.org/smash/get/diva2:839692/FULLTEXT01.pdf},
volume = {54},
year = {2015}
}
@misc{Mordvintsev2015,
author = {Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
booktitle = {Google Research Blog},
file = {:Users/omegak/Documents/Papers/Mordvintsev, Olah, Tyka - 2015 - Inceptionism Going Deeper into Neural Networks.html:html},
mendeley-groups = {Thesis/Articles},
title = {{Inceptionism: Going Deeper into Neural Networks}},
url = {https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
urldate = {2016-09-03},
year = {2015}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:Users/omegak/Documents/Papers/Hinton - Unknown - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
mendeley-groups = {Thesis/Used},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Nguyen2014,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
file = {:Users/omegak/Documents/Papers/Nguyen, Yosinski, Clune - 2014 - Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images.pdf:pdf},
isbn = {9781467369640},
mendeley-groups = {Thesis/Articles},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
url = {http://arxiv.org/abs/1412.1897},
year = {2014}
}
@article{Nikulin2016,
abstract = {We explore the method of style transfer presented in the article "A Neural Algorithm of Artistic Style" by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge (arXiv:1508.06576). We first demonstrate the power of the suggested style space on a few examples. We then vary different hyper-parameters and program properties that were not discussed in the original paper, among which are the recognition network used, starting point of the gradient descent and different ways to partition style and content layers. We also give a brief comparison of some of the existing algorithm implementations and deep learning frameworks used. To study the style space further we attempt to generate synthetic images by maximizing a single entry in one of the Gram matrices {\$}\backslashmathcal{\{}G{\}}{\_}l{\$} and some interesting results are observed. Next, we try to mimic the sparsity and intensity distribution of Gram matrices obtained from a real painting and generate more complex textures. Finally, we propose two new style representations built on top of network's features and discuss how one could be used to achieve local and potentially content-aware style transfer.},
archivePrefix = {arXiv},
arxivId = {1602.07188},
author = {Nikulin, Yaroslav and Novak, Roman},
eprint = {1602.07188},
file = {:Users/omegak/Documents/Papers/Nikulin, Novak - 2016 - Exploring the Neural Algorithm of Artistic Style.pdf:pdf},
journal = {CoRR},
mendeley-groups = {Thesis/Articles},
pages = {14},
title = {{Exploring the Neural Algorithm of Artistic Style}},
url = {http://arxiv.org/abs/1602.07188},
year = {2016}
}
@incollection{Orr2008,
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Orr, Genevieev B. and Muller, Kalus-Robert},
booktitle = {Neural Networks: Tricks of the Trade},
chapter = {Efficient BackProp},
doi = {10.1007/978-3-642-29066-4{_}11},
eprint = {9780201398298},
file = {:Users/omegak/Documents/Papers/Orr, Muller - 2008 - Neural Networks Tricks of the Trade.pdf:pdf},
isbn = {978-3-642-29065-7},
issn = {03029743},
keywords = {Whole Genome Duplication,duplication mutations,e},
mendeley-groups = {Thesis/Articles},
pages = {122--133},
pmid = {4520227},
title = {{Neural Networks: Tricks of the Trade}},
url = {http://dx.doi.org/10.1007/978-3-642-29066-4{\_}11},
volume = {7246},
year = {2008}
}
@article{Parker1985,
author = {Parker, David B},
mendeley-groups = {Thesis/Used},
title = {{Learning logic}},
year = {1985}
}
@article{Pinto2008,
abstract = {Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, "natural" images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled "natural" images in guiding that progress. In particular, we show that a simple V1-like model--a neuroscientist's "null" model, which should perform poorly at real-world visual object recognition tasks--outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a "simpler" recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition--real-world image variation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.2745v1},
author = {Pinto, Nicolas and Cox, David D. and DiCarlo, James J.},
doi = {10.1371/journal.pcbi.0040027},
eprint = {arXiv:1202.2745v1},
file = {:Users/omegak/Documents/Papers/Cox et al. - 2016 - Why is Real-World Visual Object Recognition Hard The Harvard community has made this article openly Why is Real-Wor.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
mendeley-groups = {Thesis/Articles},
number = {1},
pages = {0151--0156},
pmid = {18225950},
title = {{Why is real-world visual object recognition hard?}},
url = {https://dash.harvard.edu/bitstream/handle/1/11213329/2211529.pdf?sequence=1},
volume = {4},
year = {2008}
}
@article{Portilla2000,
abstract = {We present a universal statistical model for texture images in the context of an overcomplete complex wavelet transform. The model is parameterized by a set of statistics computed on pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. We develop an efficient algorithm for synthesizing random images subject to these constraints, by iteratively projecting onto the set of images satisfying each constraint, and we use this to test the perceptual validity of the model. In particular, we demonstrate the necessity of subgroups of the parameter set by showing examples of texture synthesis that fail when those parameters are removed from the set. We also demonstrate the power of our model by successfully synthesizing examples drawn from a diverse collection of artificial and natural textures. Vision is the process of extracting information from the images that enter the eye. The set of all possible images is vast, and yet only a small fraction of these are likely to be encountered in a natural setting (Kersten, 1987; Field, 1987; Daugman, 1989; Ruderman and Bialek, 1994). Nevertheless, it has proven difficult to character-ize this set of " natural " images, using either determin-istic or statistical models. The class of images that we commonly call " visual texture " seems most amenable to statistical modeling. Loosely speaking, texture im-ages are specially homogeneous and consist of repeated elements, often subject to some randomization in their location, size, color, orientation, etc. Julesz pioneered the statistical characterization of textures by hypoth-esizing that the N th-order joint empirical densities of image pixels (for some unspecified N), could be used to partition textures into classes that are preattentively indistinguishable to a human observer (Julesz, 1962). This work established the description of texture using homogeneous (stationary) random fields, the goal of determining a minimal set of statistical measurements for characterization, and the validation of texture mod-els through human perceptual comparisons. Julesz et al. later proposed that pairwise (N = 2) statistics were sufficient (Julesz et al., 1973), but then disproved this conjecture by producing example pairs of textures with identical statistics through second (and even third) or-der that were visually distinct (Caelli and Julesz, 1978; Julesz et al., 1978). Since then, two important developments have en-abled a new generation of more powerful statistical tex-ture models. The first is the theory of Markov random fields, in which the full model is characterized by statis-tical interactions within local neighborhoods. A num-ber of authors have developed Markov texture models, along with tools for characterizing and sampling from such models (e.g. Hassner and Sklansky, 1980; Cross and Jain, 1983; Geman and Geman, 1984; Derin and Elliott, 1987). The second is the use of oriented lin-ear kernels at multiple spatial scales for image analysis and representation. The widespread use of such kernels as descriptions of early visual processing in mammals inspired a large number of models for texture classi-fication and segmentation (e.g., Bergen and Adelson,},
author = {Portilla, Javier and Simoncelli, Eero P.},
doi = {10.1023/A:1026553619983},
file = {:Users/omegak/Documents/Papers/Portilla, Simoncelli - 2000 - A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {alternating projec-,eye,images that enter the,julesz conjecture,markov random field,non gaussian statistics,of extracting information from,possible images,texture modeling,texture synthesis,the,the set of all,tions,vision is the process},
mendeley-groups = {Thesis/Articles},
number = {1},
pages = {49--71},
title = {{Parametric texture model based on joint statistics of complex wavelet coefficients}},
url = {http://www.cns.nyu.edu/pub/lcv/portilla99-reprint.pdf},
volume = {40},
year = {2000}
}
@article{Riesenhuber1999,
abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
author = {Riesenhuber, M. and Poggio, T.},
doi = {10.1038/14819},
file = {:Users/omegak/Documents/Papers/Riesenhuber, Poggio - 1999 - Hierarchical models of object recognition in cortex.pdf:pdf},
isbn = {1097-6256},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Animals,Computer Simulation,Form Perception,Form Perception: physiology,Macaca,Mental Recall,Mental Recall: physiology,Models,Neurological,Neurons,Neurons: physiology,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Fields,Visual Fields: physiology},
mendeley-groups = {Thesis/Used},
number = {11},
pages = {1019--25},
pmid = {10526343},
title = {{Hierarchical models of object recognition in cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10526343},
volume = {2},
year = {1999}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {:Users/omegak/Documents/Papers/Rosenblatt - 1958 - The perceptron A probabilistic model for information storage and organization in {\ldots}.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
mendeley-groups = {Thesis/Articles},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf$\backslash$npapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@article{Ruder2016,
abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
archivePrefix = {arXiv},
arxivId = {1604.08610},
author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1604.08610},
file = {:Users/omegak/Documents/Papers/Ruder, Dosovitskiy, Brox - 2016 - Artistic style transfer for videos.pdf:pdf},
mendeley-groups = {Thesis/Articles},
pages = {1--14},
title = {{Artistic style transfer for videos}},
url = {http://arxiv.org/abs/1604.08610},
year = {2016}
}
@article{Ruck1990,
author = {Ruck, Dw and Rogers, Sk and Kabrisky, M},
file = {:Users/omegak/Documents/Papers/Ruck, Rogers, Kabrisky - 1990 - Feature selection using a multilayer perceptron.pdf:pdf},
journal = {Journal of Neural Network Computing},
mendeley-groups = {Thesis/Used},
pages = {1--14},
title = {{Feature selection using a multilayer perceptron}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6617{\&}rep=rep1{\&}type=pdf},
year = {1990}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/omegak/Documents/Papers/Unknown - 1986 - {\textcopyright} 1986 Nature Publishing Group.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {Thesis/Articles},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learning+representations+by+back-propagating+errors{\&}ots=zZDj2mGYVQ{\&}sig=mcyEACaE{\_}ZB4FB4xsoTgXgcbE2g$\backslash$nhttp://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learni},
volume = {323},
year = {1986}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.0575v3.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Samuel1959,
abstract = {Two machine-learning procedures have been investigated 1 in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Further- more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
author = {Samuel, Artur L},
doi = {10.1147/rd.33.0210},
file = {:Users/omegak/Documents/Papers/B, G-, Samuel - Unknown - Some Studies in Machine Learning.pdf:pdf},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of research and development},
mendeley-groups = {Thesis/Used},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
volume = {3},
year = {1959}
}
@article{Sherrington1906,
author = {Sherrington, C S},
doi = {10.1113/jphysiol.1906.sp001139},
file = {:Users/omegak/Documents/Papers/Unknown - 1904 - OBSERVATIONS ON THE SCRATCH-REFLEX IN THE SPINAL DOG. By C. S. SHERRINGTON. (27.pdf:pdf},
isbn = {0022-3751 (Print)},
issn = {0022-3751},
journal = {The Journal of physiology},
mendeley-groups = {Thesis/Used},
pages = {1--50},
pmid = {16992835},
title = {{Observations on the scratch-reflex in the spinal dog.}},
volume = {34},
year = {1906}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1409.1556.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {ImageNet Challenge},
mendeley-groups = {Thesis,Used in thesis},
pages = {1--10},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Simonyan2014B,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {:Users/omegak/Documents/Papers/Simonyan, Vedaldi, Zisserman - 2014 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
journal = {Iclr},
mendeley-groups = {Thesis/Articles},
pages = {1--},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2014}
}
@misc{Simonyan2014web,
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {GitHub},
file = {:Users/omegak/Documents/Papers/Simonyan, Zisserman - 2014 - ILSVRC-2014 model (VGG team) with 19 weight layers.html:html},
mendeley-groups = {Thesis/Articles},
title = {{ILSVRC-2014 model (VGG team) with 19 weight layers}},
url = {https://gist.github.com/ksimonyan/3785162f95cd2d5fee77},
urldate = {2016-08-30},
year = {2014}
}
@misc{Socher2015,
author = {Socher, Richard},
booktitle = {cs224d.stanford.edu},
file = {:Users/omegak/Documents/Papers/Socher - 2015 - Vanishing Gradients.html:html},
mendeley-groups = {Thesis/Articles},
title = {{Vanishing Gradients}},
url = {https://cs224d.stanford.edu/notebooks/vanishing{\_}grad{\_}example.html},
urldate = {22/08/2016},
year = {2015}
}
@article{Tenenbaum1997,
abstract = {We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor struc- ture. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMsj, which generalizes earlier work on factorial mixture models [7, 3]. Significant per- formance improvement on a benchmark speech dataset shows the benefits of our approach.},
author = {Tenenbaum, J B and Freeman, W T},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/1290-separating-style-and-content.pdf:pdf},
journal = {Advances in neural information processing},
title = {{Separating style and content with bilinear models.}},
volume = {9},
year = {1997}
}
@article{Tenenbaum2000,
abstract = {Perceptual systems routinely separate "content" from "style," classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
author = {Tenenbaum, J B and Freeman, W T},
doi = {10.1162/089976600300015349},
file = {:Users/omegak/Desktop/10.1162@089976600300015349.pdf:pdf},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1247--1283},
pmid = {10935711},
title = {{Separating style and content with bilinear models}},
volume = {12},
year = {2000}
}
@misc{Thagard2008,
author = {Thagard, Paul},
booktitle = {The Stanford Encyclopedia of Philosophy},
editor = {Zalta, Edward N.},
mendeley-groups = {Thesis/Articles},
title = {{Cognitive Science}},
year = {2008}
}
@article{Thorpe1989,
abstract = {Page 1. 1 BIOLOGICAL CONSTRAINTS ON CONNECTIONIST MODELLING Simon J. Thorpe and Michel Imbert, Institut des Neurosciences, Departement des Neurosciences de la Vision, Universit{\'{e}} Pierre et Marie Curie, 9, Quai St. Bernard, 75005, Paris, FRANCE. ABSTRACT},
author = {Thorpe, Simon J. and Imbert, Michel},
doi = {10.1.1.96.6484},
file = {:Users/omegak/Documents/Papers/Thorpe - Unknown - Biological constraints on connectionist modeling.pdf:pdf},
journal = {Connectionism in perspective},
keywords = {vision},
mendeley-groups = {Thesis/Articles},
number = {August 2016},
pages = {1--36},
title = {{Biological constraints on connectionist modelling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.6484{\&}rep=rep1{\&}type=pdf},
year = {1989}
}
@article{Ulyanov2016,
abstract = {Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys{\~{}}et{\~{}}al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.},
archivePrefix = {arXiv},
arxivId = {1603.03417},
author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
eprint = {1603.03417},
file = {:Users/omegak/Documents/Papers/Ulyanov et al. - 2016 - Texture Networks Feed-forward Synthesis of Textures and Stylized Images.pdf:pdf},
journal = {CoRR},
mendeley-groups = {Thesis/Articles},
title = {{Texture Networks: Feed-forward Synthesis of Textures and Stylized Images}},
url = {http://arxiv.org/abs/1603.03417},
year = {2016}
}
@article{Upchurch2016,
abstract = {We propose a new neural network architecture for solving single-image analogies - the generation of an entire set of stylistically similar images from just a single input image. Solving this problem requires separating image style from content. Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances. On the challenging task of generating a 62-letter font from a single example letter we produce images with 22.4{\%} lower dissimilarity to the ground truth than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1603.02003},
author = {Upchurch, Paul and Snavely, Noah and Bala, Kavita},
eprint = {1603.02003},
file = {:Users/omegak/Documents/Papers/Upchurch, Snavely, Bala - 2016 - From A to Z Supervised Transfer of Style and Content Using Deep Neural Network Generators.pdf:pdf},
mendeley-groups = {Thesis/Articles},
title = {{From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators}},
url = {http://arxiv.org/abs/1603.02003},
year = {2016}
}
@article{Visin2015,
abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
archivePrefix = {arXiv},
arxivId = {1505.00393},
author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua},
eprint = {1505.00393},
file = {:Users/omegak/Documents/Papers/Visin et al. - 2015 - ReNet A Recurrent Neural Network Based Alternative to Convolutional Networks.pdf:pdf},
journal = {Arxiv},
mendeley-groups = {Thesis/Used},
pages = {1--9},
title = {{ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks}},
url = {http://arxiv.org/abs/1505.00393},
year = {2015}
}
@article{Werbos1974,
author = {Werbos, Paul},
mendeley-groups = {Thesis/Used},
title = {{Beyond regression: New tools for prediction and analysis in the behavioral sciences}},
year = {1974}
}
@article{Wilkniss1998,
author = {Wilkniss, Sandra and Davis, Kristin},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/p292-lin.pdf:pdf},
isbn = {026206197X},
issn = {1095-158X},
journal = {Psychiatric rehabilitation journal},
mendeley-groups = {Thesis,Used in thesis},
number = {3},
pages = {244--8},
pmid = {20061264},
title = {{Book reviews.WordNet: An Electronic Lexical Database}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20061264},
volume = {33},
year = {1998}
}
@article{Xie2007,
author = {Xie, Xuexiang and Tian, Feng and Seah, Hock Soon},
doi = {10.1145/1306813.1306830},
file = {:Users/omegak/Documents/Papers/Xie, Tian, Seah - 2007 - Feature Guided Texture Synthesis ( FGTS ) for Artistic Style Transfer.pdf:pdf},
isbn = {9781595937087},
journal = {Proceedings of the 2nd international conference on Digital interactive media in entertainment and arts - DIMEA '07},
keywords = {artistic style transfer,figure 1,non-photorealistic rendering,perceptual similarity,texture synthesis,texture transfer,the stylized image},
mendeley-groups = {Thesis/Articles},
pages = {44},
title = {{Feature Guided Texture Synthesis (FGTS) for artistic style transfer}},
url = {http://portal.acm.org/citation.cfm?doid=1306813.1306830},
year = {2007}
}
@article{Yamins2016,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/omegak/Dropbox/#School/6.SoftComputing/PT/Papers/yamins2016.pdf:pdf},
issn = {1097-6256},
keywords = {d t h e,e u r a,f o c u,l c o m,models to understand,o n a n,o r y,p u tat i,s o n n,spective,using goal-driven deep learning},
mendeley-groups = {Thesis},
number = {October 2015},
pmid = {26906502},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
year = {2016}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/omegak/Documents/Papers/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visualization.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
mendeley-groups = {Thesis/Articles},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Zadeh1994,
author = {Zadeh, Lofti A.},
doi = {10.1145/175247.175255},
file = {:Users/omegak/Documents/Papers/Zadeh - 1994 - Fuzzy logic, neural networks, and soft computing.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
mendeley-groups = {Thesis/Used},
number = {3},
pages = {77--85},
title = {{Fuzzy logic, neural networks, and soft computing}},
volume = {37},
year = {1994}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/omegak/Documents/Papers/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision–ECCV 2014},
mendeley-groups = {Thesis/Articles},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53$\backslash$nhttp://arxiv.org/abs/1311.2901$\backslash$npapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
volume = {8689},
year = {2014}
}
@article{Zhang1999,
abstract = {In this paper, we report our experiments on feature-based facial expression recognition within an architecture based on a two-layer perceptron. We investigate the use of two types of features extracted from face images:  the geometric positions of a set of fiducial points on a face, and a set of multi-scale and multi-orientation Gabor wavelet coefficients at these points.  They can be used either independently or jointly. The recognition performance with different types of features has been compared, which shows that Gabor wavelet coefficients are much more powerful than geometric positions.  Furthermore, since the first layer of the perceptron actually performs a nonlinear reduction of the dimensionality of the feature space, we have also studied the desired number of hidden units, i.e., the appropriate dimension to represent a facial expression in order to achieve a good recognition rate.   It turns out that five to seven hidden units are probably enough to represent the space of feature expressions. Then, we have investigated the importance of each individual fiducial point to facial expression recognition.  Sensitivity analysis reveals that points on cheeks and on forehead carry little useful information.  After discarding them, not only the computational efficiency increases, but also the generalization performance slightly improves. Finally, we have studied the significance of image scales. Experiments show that facial expression recognition is mainly a low frequency process, and a spatial resolution of 64 pixels  64 pixels is probably enough},
author = {Zhang, Zhengyou},
doi = {10.1142/S0218001499000495},
file = {:Users/omegak/Documents/Papers/ZHANG - 1999 - Feature-Based Facial Expression Recognition Sensitivity Analysis and Experiments With a Multilayer Perceptron.pdf:pdf},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {facial expression recognition,gabor wavelets,image scale,learning,multilayer perceptron,sensitivity analysis,},
mendeley-groups = {Thesis,Thesis/Used},
number = {06},
pages = {893--911},
title = {{Feature-Based Facial Expression Recognition: Sensitivity Analysis and Experiments With a Multilayer Perceptron}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001499000495},
volume = {13},
year = {1999}
}
@article{Zhang2016,
abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a {\{}$\backslash$em plausible{\}} color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and explore using class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward operation in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test", asking human subjects to choose between a generated and ground truth color image. Our method successfully fools humans 20$\backslash${\%} of the time, significantly higher than previous methods.},
archivePrefix = {arXiv},
arxivId = {1603.08511},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
eprint = {1603.08511},
file = {:Users/omegak/Documents/Papers/Zhang, Isola, Efros - 2016 - Colorful Image Colorization.pdf:pdf},
keywords = {cnns,colorization,vision for graphics},
mendeley-groups = {Thesis/Articles},
pages = {1--25},
title = {{Colorful Image Colorization}},
url = {http://arxiv.org/abs/1603.08511},
year = {2016}
}
@article{Zhu1994,
abstract = {L-BFGS-B is a limited memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is diicult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems, and in this case performs similarly to its predecessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77.},
author = {Zhu, Ciyou and Byrd, Richard and Lu, Peihuang and Nocedal, Jorge},
file = {:Users/omegak/Documents/Papers/Zhu et al. - 1994 - L-BFGS-B - Fortran Subroutines for Large-Scale Bound Constrained Optimization.pdf:pdf},
mendeley-groups = {Thesis/Articles},
pages = {1--17},
title = {{L-BFGS-B - Fortran Subroutines for Large-Scale Bound Constrained Optimization}},
url = {http://hod.greeley.org/papers/Unsorted/lbfgsb.pdf},
year = {1994}
}
@article{Zhu2008,
abstract = {6.1 RegularizationbyGraph . . . . . . . . . . . . . . . . . . . . . . 18 6.1.1 Mincut . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 6.1.2 Discrete Markov Random Fields: Boltzmann Machines . . 19 6.1.3 Gaussian Random Fields and Harmonic Functions . . . . 19 6.1.4 LocalandGlobalConsistency . . . . . . . . . . . . . .},
author = {Zhu, Xiaojin},
doi = {10.1.1.146.2352},
file = {:Users/omegak/Documents/Papers/Zhu - 2008 - Semi-Supervised Learning Literature Survey Contents.pdf:pdf},
isbn = {0769529321},
journal = {SciencesNew York},
mendeley-groups = {Thesis/Articles},
number = {1530},
pages = {10},
pmid = {22175947},
title = {{Semi-Supervised Learning Literature Survey Contents}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681{\&}rep=rep1{\&}type=pdf},
volume = {10},
year = {2008}
}



% -- Images --------------------------------------------------------------------

@image{AI4562013,
author = {AI456},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/AI456 - 2013 - Error surface of a linear neuron with two input weights.png:png},
mendeley-groups = {Thesis/Images},
title = {{Error surface of a linear neuron with two input weights}},
url = {https://upload.wikimedia.org/wikipedia/commons/6/6d/Error{\_}surface{\_}of{\_}a{\_}linear{\_}neuron{\_}with{\_}two{\_}input{\_}weights.png},
urldate = {21/08/2016},
year = {2013}
}
@image{Aphex342015,
abstract = {Input volume connected to a convolutional layer},
author = {Aphex34},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/Aphex34 - 2015 - Convolutional Layer.html:html},
mendeley-groups = {Thesis/Images},
title = {{Convolutional Layer}},
url = {https://en.wikipedia.org/wiki/File:Conv_layer.png},
urldate = {2016-08-04}
}
@image{Apple,
author = {Apple},
booktitle = {developer.apple.com},
file = {:Users/omegak/Documents/Papers/Apple - Unknown - Kernel Convolution.jpg:jpg},
mendeley-groups = {Thesis/Images},
title = {{Kernel Convolution}},
url = {https://developer.apple.com/library/prerelease/content/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html},
urldate = {08/08/2016}
}
@image{Goodspeed2015,
author = {Goodspeed, Elizabeth},
booktitle = {Wikipedia},
file = {:Users/omegak/Documents/Papers/Goodspeed - 2015 - Perceptron Training.png:png},
mendeley-groups = {Thesis/Images},
title = {{Perceptron Training}},
url = {https://en.wikipedia.org/wiki/File:Perceptron{\_}example.svg},
urldate = {2016-08-16},
year = {2015}
}
@image{Guerzhoy2016,
author = {Guerzhoy, Michael},
booktitle = {University of Toronto},
file = {:Users/omegak/Documents/Papers/Guerzhoy - 2016 - Modern ConvNet Architectures.pdf:pdf},
mendeley-groups = {Thesis/Images},
title = {{Modern ConvNet Architectures}},
url = {http://www.cs.toronto.edu/~guerzhoy/321/lec/W06/convnets.pdf},
urldate = {2016-08-06}
}
@image{Honorio2013,
author = {Honório, Vinícius Gonçalves and Maria, Maltarollo Káthia and da Silva, Albérico Borges Ferreira},
booktitle = {intechopen.com},
doi = {10.5772/51275},
file = {:Users/omegak/Documents/Papers/Honório, Maria, Silva - 2013 - Human Neuron vs Artificial Neuron.png:png},
mendeley-groups = {Thesis/Images},
title = {{Human Neuron vs Artificial Neuron}},
url = {http://www.intechopen.com/books/artificial-neural-networks-architectures-and-applications/applications-of-artificial-neural-networks-in-chemical-problems},
year = {2013}
}
@image{Karpathy,
author = {Karpathy, Andrej},
booktitle = {Stanford Computer Science Class},
file = {:Users/omegak/Documents/Papers/Karpathy - Unknown - General Pool.jpeg:jpeg},
mendeley-groups = {Thesis/Images},
title = {{General Pool}},
url = {http://cs231n.github.io/convolutional-networks/#pool},
urldate = {2016-08-07}
}
@image{Karpathya,
author = {Karpathy, Andrej},
booktitle = {Stanford Computer Science Class},
file = {:Users/omegak/Documents/Papers/Karpathy - Unknown - Maxpool.jpeg:jpeg},
mendeley-groups = {Thesis/Images},
title = {{Maxpool}},
url = {http://cs231n.github.io/convolutional-networks/#pool},
urldate = {2016-08-07}
}
@image{Medina2013,
author = {Medina, Gonzalo},
booktitle = {tex.stackexchange.com},
file = {:Users/omegak/Documents/Papers/Medina - 2013 - Neural Network in Tikz.html:html},
mendeley-groups = {Thesis/Images},
title = {{Neural Network in Tikz}},
url = {http://tex.stackexchange.com/a/132471},
urldate = {15/08/2016},
year = {2013}
}
@image{Medina2013A,
author = {Medina, Gonzalo},
booktitle = {tex.stackexchange.com},
file = {:Users/omegak/Documents/Papers/Medina - 2013 - Multilayer Perceptron in Tikz.html:html},
mendeley-groups = {Thesis/Images},
title = {{Multilayer Perceptron in Tikz}},
url = {http://tex.stackexchange.com/a/132471},
urldate = {2016-08-18},
year = {2013}
}
@image{Wolf,
author = {Wolf, Lior},
booktitle = {University of Tel Aviv},
file = {:Users/omegak/Documents/Papers/Wolf - Unknown - Object Recognition.jpg:jpg},
mendeley-groups = {Thesis/Images},
title = {{Object Recognition}},
url = {http://www.cs.tau.ac.il/{~}wolf/OR2/},
urldate = {04/09/2016}
}
